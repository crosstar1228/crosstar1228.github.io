<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://crosstar1228.github.io/tag/cs231n/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://crosstar1228.github.io/" rel="alternate" type="text/html" />
  <updated>2021-10-13T13:49:51+00:00</updated>
  <id>https://crosstar1228.github.io/tag/cs231n/feed.xml</id>

  
  
  

  
    <title type="html">건너별의 Romantic AI | </title>
  

  
    <subtitle>IT/인공지능 서랍장</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">CS231n Lecture 12. Visualizing and understanding</title>
      <link href="https://crosstar1228.github.io/cs231n-lec12" rel="alternate" type="text/html" title="CS231n Lecture 12. Visualizing and understanding" />
      <published>2021-09-16T11:00:00+00:00</published>
      <updated>2021-09-16T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/cs231n-lec12</id>
      <content type="html" xml:base="https://crosstar1228.github.io/cs231n-lec12">&lt;h1 id=&quot;goal&quot;&gt;GOAL&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;시각화를 통해 CNN의 feature를 이해하는 방법론들을 activation과 gradient의 관점에서 알아본다.&lt;/li&gt;
  &lt;li&gt;이미지 Style을 변형시키는 방법에 대해 알아본다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#deepdream&quot;&gt;Deepdream&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#feature-inversion&quot;&gt;Feature Inversion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#texture-synthesis&quot;&gt;Texture Synthesis&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#gram-matrix&quot;&gt;Gram Matrix&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#style-transfer&quot;&gt;Style Transfer&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#fast-style-transfer&quot;&gt;Fast Style Trnasfer&lt;/a&gt;
        &lt;h2 id=&quot;layers&quot;&gt;Layers&lt;/h2&gt;
      &lt;/li&gt;
      &lt;li&gt;first layer&lt;/li&gt;
      &lt;li&gt;last layer -&amp;gt; feature nearest&lt;/li&gt;
      &lt;li&gt;t-SNE dimensionality reduction
        &lt;h2 id=&quot;deepdream&quot;&gt;Deepdream&lt;/h2&gt;
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; - [위키백과 : Deepdream](https://en.wikipedia.org/wiki/DeepDream)  
 - [이미지 사이트 체험](https://deepdreamgenerator.com/)
 - Google 에서 만든 재미용
 - network의 중간에 **특정한 neuron activation을 Amplify(증폭)**  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;-&amp;gt; 네트워크가 이미 뽑아낸 특징들을 더욱 증폭시키는 역할&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;원리
    &lt;ol&gt;
      &lt;li&gt;forward 방향으로 activation 계산 (여기까진 일반적 네트워크와 같음)&lt;/li&gt;
      &lt;li&gt;layer의 &lt;strong&gt;&lt;em&gt;activation 과 같은&lt;/em&gt; gradient를 설정&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Backward 방향으로 gradient 계산 (backprop)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;결과를 위해 조정한 사항들
    &lt;ol&gt;
      &lt;li&gt;Jitter : 이미지를 두 픽셀씩 옮김&lt;/li&gt;
      &lt;li&gt;Normalize ascent to gradient&lt;/li&gt;
      &lt;li&gt;Clip pixel values :최대,최솟값으로 제한&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;-&amp;gt; training 되었던 image들의 feature들이 input image에 함께 뒤섞여 나타남
&lt;img src=&quot;../../assets/built/images/img_1.png&quot; alt=&quot;img_1.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;feature-inversion&quot;&gt;feature Inversion&lt;/h2&gt;

&lt;p&gt;이미지의 CNN feature Vector 가 주어졌을 때&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;자연스럽고&lt;/li&gt;
  &lt;li&gt;그 feature vector 가장 잘 부합하는&lt;br /&gt;
&lt;strong&gt;이미지 output return&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/img_2.png&quot; alt=&quot;img_2.png&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;주어진 feature vector와&lt;/li&gt;
  &lt;li&gt;새로운 이미지의 feature와의 L2 distance 측정 &lt;br /&gt;
Regularization 진행 후 &lt;strong&gt;최솟값을 return&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/img_3.png&quot; alt=&quot;img_3.png&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;VGG16의 각기 다른 layer로부터 feature inversion 진행 결과,&lt;br /&gt;
  -&amp;gt; 얕은 layer보다는 깊은 layer에서 더 새로운 output이 탄생함을 알 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;texture-synthesis&quot;&gt;Texture Synthesis&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Sample patch(이미지 일부)로부터 같은 texture의 더 큰 이미지 합성하는 방식&lt;/li&gt;
  &lt;li&gt;nearest neighbor 알고리즘으로 ‘복붙’하면 깨짐 현상이 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gram-matrix&quot;&gt;Gram matrix&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;image의 질감의 유사도를 표현하는 행렬, 이를 통해 texture synthesis 진행&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/img_4.png&quot; alt=&quot;img_4.png&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;원리&lt;br /&gt;
1) image input을 CNN에 넣으면 C*H*W tensor가 출력됨&lt;br /&gt;
2) 이 tensor 중 2개의 C차원 vector를 외적하면 C*C의 co-occurence를 표현하는 matrix가 나오는데 이것이 바로 Gram matrix&lt;br /&gt;
3) 마치 covariance을 판단하는 행렬 -&amp;gt; 질감의 유사도를 측정 가능!&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gram-matrix-이용한-합성과정&quot;&gt;Gram matrix 이용한 합성과정&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/built/images/img_5.png&quot; alt=&quot;img_5.png&quot; /&gt;
1) 하나는 input으로부터, 하나는 Noise로부터 이미지를 생성&lt;br /&gt;
2) Gram matrix 추출 후 Gram matrix 간에 L2 distance의 가중합, 즉 Loss 계산&lt;br /&gt;
3) Gradient 계산을 위한 Backprop 및 parameter update(make gradient step)&lt;br /&gt;
4) 2)~3) 반복
&lt;img src=&quot;../../assets/built/images/img9.png&quot; alt=&quot;img9.png&quot; /&gt;
-&amp;gt; 이 기법 역시 깊은 Layer로부터 더 많은 feature들이 추출됨을 알 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;style-transfer&quot;&gt;Style Transfer&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;비슷한 방법으로 두 이미지의 texture 차이를 최소화함으로서 특정 이미지의 분위기 합성이 가능
-&amp;gt; Style transfer&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gram matrix값 최소화시키면서 생성하는 이미지 기법
&lt;img src=&quot;../../assets/built/images/img_6.png&quot; alt=&quot;img_6.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Content image, Style image&lt;/strong&gt;
1) content image로부터 feature reconstruction&lt;br /&gt;
2) style image로부터 gram matrix reconstruction&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;특징&lt;/strong&gt;
1) Fine control 가능(style image 크기 조정 등)&lt;br /&gt;
2) forward/backward prop 과정이 너무 많아서 느림.&lt;/p&gt;

    &lt;p&gt;-&amp;gt; style transfer를 진행하는 또 하나의 Neural Network를 구현하자!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fast-style-transfer&quot;&gt;Fast style Transfer&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/built/images/img_8.png&quot; alt=&quot;img_8.png&quot; /&gt;
1) style을 미리 &lt;strong&gt;Feedforward Network \(f_W\) 에 학습&lt;/strong&gt;시킴&lt;br /&gt;
2) content image만 feed forward -&amp;gt; &lt;strong&gt;속도 향상&lt;/strong&gt;&lt;br /&gt;
   ** batch normalization보다 instant normalization 에서 더 큰 상승 효과&lt;br /&gt;
    &lt;img src=&quot;../../assets/built/images/img9.png&quot; alt=&quot;img.png&quot; /&gt;
3) segmentation network는 여러 층으로 transposed convolution 이용해서 down,upsampling&lt;/p&gt;

&lt;h3 id=&quot;deepdream-website에서-style-transfer-통해-쇠라풍-그림으로-교체한-사진&quot;&gt;Deepdream website에서 Style transfer 통해 쇠라풍 그림으로 교체한 사진&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/built/images/img_7.png&quot; alt=&quot;img_7.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;CS231n 12강 Lecture Note : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture12.pdf&lt;/li&gt;
  &lt;li&gt;https://inhovation97.tistory.com/28&lt;/li&gt;
  &lt;li&gt;style transfer : https://www.popit.kr/neural-style-transfer-%EB%94%B0%EB%9D%BC%ED%95%98%EA%B8%B0/&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="cs231n" />
      

      
        <summary type="html">GOAL 시각화를 통해 CNN의 feature를 이해하는 방법론들을 activation과 gradient의 관점에서 알아본다. 이미지 Style을 변형시키는 방법에 대해 알아본다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">CS231n Lecture 3. Neural Network</title>
      <link href="https://crosstar1228.github.io/cs231n-lec2" rel="alternate" type="text/html" title="CS231n Lecture 3.  Neural Network" />
      <published>2021-09-15T11:00:00+00:00</published>
      <updated>2021-09-15T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/cs231n-lec2</id>
      <content type="html" xml:base="https://crosstar1228.github.io/cs231n-lec2">&lt;h1 id=&quot;안녕&quot;&gt;안녕&lt;/h1&gt;
&lt;p&gt;cs231n&lt;/p&gt;

&lt;p&gt;강의 잘하고싶다.~~~~~~~~~~!!!!!!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;진짜로.&lt;/strong&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;후&lt;/code&gt;…&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="cs231n" />
      

      
        <summary type="html">안녕 cs231n</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">CS231n Lecture 2. Linear Classifier</title>
      <link href="https://crosstar1228.github.io/cs231n-lec1" rel="alternate" type="text/html" title="CS231n Lecture 2.  Linear Classifier" />
      <published>2021-09-15T11:00:00+00:00</published>
      <updated>2021-09-15T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/cs231n-lec1</id>
      <content type="html" xml:base="https://crosstar1228.github.io/cs231n-lec1">&lt;h1 id=&quot;goal&quot;&gt;GOAL&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Data-Driven Approach로 image classification을 진행하게 된 배경을 이해합니다.&lt;/li&gt;
  &lt;li&gt;이미지를 어떻게 비교 및 분류하는지 알아보고, &lt;strong&gt;KNN(K-Nearest Neighbor)&lt;/strong&gt;에 관하여 이해합니다.&lt;/li&gt;
  &lt;li&gt;이미지를 분류하는 Linear 한 모델에 관하여 가볍게 이해해 봅니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#image-classification&quot;&gt;Image Classifciation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#data-driven-approach&quot;&gt;Data-Driven Approach&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#distance-metric&quot;&gt;Distance Metric&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#L1,-L2-distance&quot;&gt;L1, L2 Distance&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#k-nearest-neighbor&quot;&gt;K-Nearest Neighbor&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#hyperparameter&quot;&gt;Hyperparameter&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#linear-classifier&quot;&gt;Linear Classifier&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#limitations&quot;&gt;Limitations&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;image-classification&quot;&gt;Image Classification&lt;/h2&gt;
&lt;p&gt;Computer Vision에서 Image Classification은 매우 핵심적이고 근본적인 작업입니다. 하지만 우리가 직관적으로 인식하는 것이랑, 컴퓨터가 인식하는 것이랑은 차이가 있죠. 컴퓨터는 모든 것을 숫자로 받아들이기 때문입니다. 우리는 이것을 ‘Semantic Gap(의미론적 차이)’라고 표현하며, 아래 그림을 보며이해해 보겠습니다.&lt;br /&gt;
&lt;img src=&quot;../../assets/built/images/cs2_img.png&quot; alt=&quot;cs2_img.png&quot; /&gt;
컴퓨터에서 이미지는 기본적으로 0~255사이의 pixel로 표현되며, 3개의 channel의 matrix형태로 표현됩니다.&lt;br /&gt;
 -&amp;gt; 이런 숫자로부터 우리는 &lt;strong&gt;‘이 사진이 고양이다’라는 의미를 추출해 내고 싶은 게 목적&lt;/strong&gt;입니다.
하지만 빛, 변형, 보호색, 개체의 변형 등 많은 Hurdle이 존재하기에, 명백한 방법이 없었죠. 가장자리 모서리를 따라 outline을 만들어내며 추출하는 시도들이 있었지만 쉽지 않았습니다.
그래서 고안된 방법이 Data에 기반한 접근법입니다.&lt;/p&gt;

&lt;h2 id=&quot;data-driven-approach&quot;&gt;Data-Driven Approach&lt;/h2&gt;
&lt;p&gt;간단한 순서는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;1) 이미지와 label(이미지의 정답)정보가 포함된 dataset을 모으고&lt;br /&gt;
2) 머신러닝으로 training 시키고&lt;br /&gt;
3) 새로운 이미지에 대한 classifier(분류기)를 평가해 보는 것&lt;br /&gt;
&lt;img src=&quot;../../assets/built/images/cs2_img_1.png&quot; alt=&quot;cs2_img_1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;-&amp;gt; training set을 label과 함께 학습하면, 그 이후의 새로운 이미지에 대하여 분류를 통해 의미를 추출해 낼 수 있는 것이죠.
그렇다면 어떻게 기존 학습된 데이터와 새로운 이미지를 비교할 것이냐? 하는 의문이 제기되는데,
기본적으로 새로운 이미지와 기존 학습된 이미지 간의 거리를 재어 가장 거리가 가까운 이미지를 고르게 됩니다.
이것을 nearest neighbor(가장 가까이 있는 이웃)방법이라고 합니다.&lt;br /&gt;
아래 그림은 nearest neighbor에 대해 10개의 class에 따라 각각 예측이 되는 과정을 보여줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/cs2_img_2.png&quot; alt=&quot;cs2_img_2.png&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;distance-metric&quot;&gt;Distance Metric&lt;/h2&gt;
&lt;p&gt;가깝고 먼 거리(Distance)를 측정하려면 기준이 필요합니다. 강의에서는 아래 두 가지 기준을 설명해주고 있습니다.&lt;/p&gt;
&lt;h3 id=&quot;l1-l2-distance&quot;&gt;L1, L2 Distance&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/built/images/cs2_img_4.png&quot; alt=&quot;cs2_img_4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;L1 distance : 각 pixel값의 차이를 구한 후 결괏값을 합산하는 방법&lt;br /&gt;
L2 distance : 각 pixel값의 차이를 제곱한 후 root를 씌운 후 결괏값을 합산하는 방법&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;L2 distance는 root를 씌우지 않는 것으로 정의되기도 합니다.&lt;/li&gt;
  &lt;li&gt;L1과 L2 distance와 관련한 추가적인 이해는 &lt;a href=&quot;https://junklee.tistory.com/29&quot;&gt;링크&lt;/a&gt;를 참고해 주세요.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;L1 distance 방법을 통하여 거리를 계산한 아래 예시를 보며 이해해 봅시다.&lt;/p&gt;

&lt;h2&gt;&lt;img src=&quot;../../assets/built/images/cs2_img_3.png&quot; alt=&quot;cs2_img_3.png&quot; /&gt;&lt;/h2&gt;

&lt;h3 id=&quot;k-nearest-neighbor&quot;&gt;K-Nearest Neighbor&lt;/h3&gt;
&lt;p&gt;한마디로 가장 가까운 K개를 비교해 보자! 입니다. k=5라면, 가장 거리가 가까운 5개중에서 다수결로 예측을 진행하는 것이에요!
아래 코드를 살펴보며 이해해 볼까요?
{gist}&lt;/p&gt;

&lt;p&gt;하지만 이는 단점이 있습니다. 
1) 전체 학습된 데이터셋과 비교하기 때문에 너무 느리고,&lt;br /&gt;
2) outlier에 강건하지 못합니다.  &lt;br /&gt;
아래 그림을 보시죠!
&lt;img src=&quot;../../assets/built/images/cs2_img_5.png&quot; alt=&quot;cs2_img_5.png&quot; /&gt;
점들은 각각의 data를 의미하고, 색깔은 KNN에 의해 분류된 결과를 의미합니다.
예를 들어, 초록색이 고양이를 의미하고 노란색이 강아지를 의미한다면, 초록색 영역 안에 포함된 data들이 고양이로 분류된 것이죠.
하지만 조금 이상한 점은, 중간에 섬처럼 떨어져 있는 노란색 지점입니다. 초록색으로 분류되는 것이 자연스러움에도 불구하고, distance가 더 가깝다는 이유만으로 강아지로 분류된 것입니다. 실제로 data를 확인해 봤을 때, 이것은 고양이일 확률이 높고, KNN 알고리즘이 잘못 예측하였을 확률이 높습니다.
(다시 말해, 강아지라고 학습된 데이터와 거리가 가깝다는 이유만으로, 실제로 고양이로 분류되는 것이 더 적절함에도 강아지로 분류된 것입니다.)
이러한 단점 때문에 KNN은 거의 사용되지 않습니다. 이는 또한 &lt;strong&gt;차원의 저주&lt;/strong&gt; 개념과도 연관되어 있는데, 이는 나중에 다루도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;추가로, K의 수에 따라 분류 성능 및 결과도 달라지게 되는데, &lt;a href=&quot;http://vision.stanford.edu/teaching/cs231n-demos/knn/&quot;&gt;링크&lt;/a&gt;에서 실험해보면서 이해해 보시기 바랍니다!&lt;/p&gt;

&lt;h3 id=&quot;hyperparameter&quot;&gt;Hyperparameter&lt;/h3&gt;
&lt;p&gt;그렇다면 최적의 K값은 어떻게 설정할 수 있을까요? 또, 어떠한 기준으로 거리(distance)를 측정하는 것이 보다 나은 성능을 안겨줄까요?
그것은 우리가 모델을 직접 돌려가면서 가장 좋은 성능이 나올 수 있게 조정을 해주어야 합니다(problem-dependent).&lt;br /&gt;
알고리즘 및 모델에 따라 이러한 기본적인 setting에 필요한 값을 우리는 &lt;strong&gt;hyperparameter&lt;/strong&gt;라고 부릅니다!&lt;/p&gt;

&lt;p&gt;이것은 모델이 자체적으로 학습하고 update하는 parameter와 대비됩니다. 이것에 대한 설명은 많이 할 수 있는 기회가 있을 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/cs2_img_6.png&quot; alt=&quot;cs2_img_6.png&quot; /&gt;
Hyperparameter는 총 dataset을 training, validation(생략되기도 함), test set 이렇게 세 가지로 나누어 학습을 진행하면서 조정됩니다.&lt;br /&gt;
&lt;strong&gt;분류 모델링 시 기본이 되는 구조&lt;/strong&gt;이니 잘 기억해 두시면 좋습니다!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h3 id=&quot;tip&quot;&gt;Tip🥳&lt;/h3&gt;
  &lt;p&gt;위 슬라이드를&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;trainset = 시험공부 범위&lt;/li&gt;
    &lt;li&gt;validation set = 모의고사&lt;/li&gt;
    &lt;li&gt;test set = 수능 시험&lt;br /&gt;
이라고 생각하고 한 번 이해해 보시기 바랍니다!&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/cs2_img_7.png&quot; alt=&quot;cs2_img_7.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이와 달리 trainset을 여러 Fold(subset)으로 나누어 검증(validation)하는 방법도 있습니다. 각 Fold별로 결과를 평균내어 산출합니다. 검증을 여러 번 시도할 수 있는 장점이 있겠네요!&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;linear-classifier&quot;&gt;Linear Classifier&lt;/h2&gt;

&lt;p&gt;우리는 어떻게 이미지 간 유사도를 측정하고, 이미지에서 의미를 뽑아내어 분류하는지 그 방법에 대하여 배웠습니다. 그것에 기반하여 실제 분류를 하는 작업을 살펴봅시다.&lt;br /&gt;
10개의 class로 분류하는 작업이며, 50,000개의 trainset과 10,000개의 testset으로 이루어져 있는 &lt;a href=&quot;https://www.cs.toronto.edu/~kriz/cifar.html&quot;&gt;CIFAR10&lt;/a&gt; datset입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/cs2_img_8.png&quot; alt=&quot;cs2_img_8.png&quot; /&gt;
고양이 image를 input(x)으로 넣으면, 우리가 설정한 모델(f(x,W))의 연산에 의해 10개의 class 에 대한 각각의 score(점수)를 output으로 확인하게 됩니다. 
이 점수가 가장 높은 class로 모델은 예측을 하게 되는 것이죠!&lt;br /&gt;
여기서 W는 Weight 또는 Parameter라고 하며, input으로부터 output을 반환해 주는 가중치의 역할을 하는 매우 중요한 개념입니다.&lt;br /&gt;
여기서 b는 결괏값을 우리가 원하는 모델로 근사하도록 조정해주는 값입니다. 이는 Lecture 3 에서 자세히 다뤄보겠습니다.&lt;br /&gt;
아래 슬라이드를 보며 제가 설명한 내용을 이해해 보시면 좋을 것 같습니다!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/cs2_img_9.png&quot; alt=&quot;cs2_img_9.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결과적으로, linear한 모델로부터 아래 그림과 같이 분류 작업이 이루어지게 됩니다.
&lt;img src=&quot;../../assets/built/images/cs2_img_10.png&quot; alt=&quot;cs2_img_10.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;limitations&quot;&gt;Limitations&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;비선형 모델링이 어렵습니다. 아래 그림과 같이, 비선형한 함수로부터 만들어진 class로 분류작업을 진행할 수 없죠.
&lt;img src=&quot;../../assets/built/images/cs2_img_11.png&quot; alt=&quot;cs2_img_11.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;parameter를 스스로 update하지 못합니다.&lt;/strong&gt; 우리는 이것이 좋은 모델인지 아닌지 그저 결과만 보고 판단하는 수밖에 없으니, 일일이 모델의 parameter W를 수정해 주어야 하는 것이죠.
&lt;img src=&quot;../../assets/built/images/cs2_img_12.png&quot; alt=&quot;cs2_img_12.png&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이러한 의문을 갖고 다음 강의에서 어떻게 모델을 최적화(optimize)하는지 배워보겠습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;blockquote&gt;
  &lt;h2 id=&quot;회고&quot;&gt;회고😎&lt;/h2&gt;
  &lt;ul&gt;
    &lt;li&gt;이미지의 의미론적인 정보를 추출하기 위해 data-driven approach가 고안되었고, 이는 training set을 머신러닝 기법으로 학습하는 것이다.&lt;/li&gt;
    &lt;li&gt;KNN으로 K개의 가장 distance가 작은 image와의 비교를 통해 새로운 이미지를 분류 가능하지만, 잘 쓰이지는 않는다.&lt;/li&gt;
    &lt;li&gt;linear한 classification은 모델 최적화에 어려움이 있다.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;생각해-볼-문제&quot;&gt;생각해 볼 문제!&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;hyperparameter를 처음에 값을 정하는 근거가 필요해요. 처음에 어떻게 정할까요?&lt;/li&gt;
  &lt;li&gt;k-fold validation은 왜 작은 dataset에만 유용할까요?&lt;/li&gt;
  &lt;li&gt;linear classification에서 linear의 의미는 무엇일까요?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;CS231n Lecture 2 : https://youtu.be/OoUX-nOEjG0&lt;/li&gt;
  &lt;li&gt;CS231n Lecture slide : http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture2.pdf&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="cs231n" />
      

      
        <summary type="html">GOAL Data-Driven Approach로 image classification을 진행하게 된 배경을 이해합니다. 이미지를 어떻게 비교 및 분류하는지 알아보고, KNN(K-Nearest Neighbor)에 관하여 이해합니다. 이미지를 분류하는 Linear 한 모델에 관하여 가볍게 이해해 봅니다.</summary>
      

      
      
    </entry>
  
</feed>
