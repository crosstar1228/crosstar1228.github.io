<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://crosstar1228.github.io/tag/nlp/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://crosstar1228.github.io/" rel="alternate" type="text/html" />
  <updated>2022-09-26T14:06:37+00:00</updated>
  <id>https://crosstar1228.github.io/tag/nlp/feed.xml</id>

  
  
  

  
    <title type="html">건너별의 Romantic AI | </title>
  

  
    <subtitle>IT/인공지능 서랍장</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">Symbolic Music Representation - 1D, 2D Future Directions</title>
      <link href="https://crosstar1228.github.io/NLP-music_generation_input_output" rel="alternate" type="text/html" title="Symbolic Music Representation - 1D, 2D Future Directions" />
      <published>2022-09-13T11:00:00+00:00</published>
      <updated>2022-09-13T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/NLP-music_generation_input_output</id>
      <content type="html" xml:base="https://crosstar1228.github.io/NLP-music_generation_input_output">&lt;blockquote&gt;
  &lt;p&gt;Music representation은 크게 Symbol domain 과 audio domain으로 나뉩니다. 그 중 midi 파일의 표현 방식이자, discrete한 representation을 가지는
&lt;strong&gt;Symbolic music generation&lt;/strong&gt;에 대해 알아봅시다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;symbolic-music-domain&quot;&gt;Symbolic Music Domain&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;discrete 하고 표현이 명확함&lt;/li&gt;
  &lt;li&gt;dynamic 과 같은 악보 외적인 사항을 학습할 수 있음&lt;/li&gt;
  &lt;li&gt;특정악기에 customizing되어 있어서, 새로운 악기에 기존 modeling technique를 적용하는 것이 어려움&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1d-representation&quot;&gt;1D representation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;일반적인 표현 방법&lt;/li&gt;
  &lt;li&gt;note 간 관계 표현이 어려움(이전 note가 다음 note 시작까지 지속된다 등의 정보)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1-event-based&quot;&gt;1. Event-Based&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Midi-like event
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/063a5409-734b-4782-bde0-e7d71b66c7bb/image.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-sequence-based&quot;&gt;2. Sequence-Based&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;pitch, duration, chord 와 다른 음악 정보를 여러 list형태로 표현&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2d-representation&quot;&gt;2D representation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;sampling time에 의해 note를 2D matrix로 표현&lt;/li&gt;
  &lt;li&gt;더 높은 resolution을 필요로 함
    &lt;ul&gt;
      &lt;li&gt;rhythm이 복잡해지는 등 time dimension이 높아지면 장기적 의존관계를 학습하기 어려움
        &lt;h3 id=&quot;piano-roll&quot;&gt;Piano Roll&lt;/h3&gt;
        &lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/3856593a-eecb-410a-9ead-ff81f5893df4/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Automatic Piano에 의해 영감을 받음&lt;/li&gt;
  &lt;li&gt;(pitch 수 + resting state + holding state) * (resolution)모양
    &lt;ul&gt;
      &lt;li&gt;높이는 일반적으로 130개&lt;/li&gt;
      &lt;li&gt;너비는 일반적으로&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;하나의 matrix는 하나의 track을 의미하며, 여러 개의 track은 여러 matrix로 표현됨&lt;/li&gt;
  &lt;li&gt;pretty_midi 또는 Pypianoroll python toolkit이용하여 손쉽게 변환 가능&lt;/li&gt;
  &lt;li&gt;일반적으로 binary
    &lt;ul&gt;
      &lt;li&gt;각 position은 note가 그 position에서 연주되고 있는지 아닌지를 표현&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;note-off 정보가 없어서 16분음표 2개 반복과, 8분음표 하나를 같게 인식함&lt;/li&gt;
  &lt;li&gt;이를 개선하는 방법에는
    &lt;ul&gt;
      &lt;li&gt;hold replay&lt;/li&gt;
      &lt;li&gt;time step을 note beginning과 note end로 나눔&lt;/li&gt;
      &lt;li&gt;hold 를 표현하는 symbol인 ‘_’ 를 사용(monophonic melody에만 가능)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Conlon pianoroll : CNN에서 receptive field가 크지 않아도 되는 장점&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;A Comprehensive Survey on Deep Music Generation(2020)&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="nlp" />
      

      
        <summary type="html">Music representation은 크게 Symbol domain 과 audio domain으로 나뉩니다. 그 중 midi 파일의 표현 방식이자, discrete한 representation을 가지는 Symbolic music generation에 대해 알아봅시다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">A Comprehensive Survey on Deep Music Generation - Multi-level Representations, Algorithms, Evaluations, and Future Directions</title>
      <link href="https://crosstar1228.github.io/NLP-music_generation_paper_review_1" rel="alternate" type="text/html" title="A Comprehensive Survey on Deep Music Generation - Multi-level Representations, Algorithms, Evaluations, and Future Directions" />
      <published>2022-08-27T11:00:00+00:00</published>
      <updated>2022-08-27T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/NLP-music_generation_paper_review_1</id>
      <content type="html" xml:base="https://crosstar1228.github.io/NLP-music_generation_paper_review_1">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;딥러닝을 활용한 최근의 다양한 음악 작곡 TASK를 소개한 논문
    &lt;ul&gt;
      &lt;li&gt;DATASETS&lt;/li&gt;
      &lt;li&gt;MUSIC REPRESENTATION&lt;/li&gt;
      &lt;li&gt;EVALUATION METHOD&lt;/li&gt;
      &lt;li&gt;challenges &amp;amp; future directions&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;intro&quot;&gt;Intro&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;음악은 언어와 같다.&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;악보 : 음악  == 글 : 말&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Music Generation의 대분류 3개
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/33ed8e71-a0d8-4853-9c90-c9f1940900dc/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;score generation&lt;/li&gt;
      &lt;li&gt;performance generation add performance characteristics to scores
        &lt;ul&gt;
          &lt;li&gt;RENDERING performance&lt;/li&gt;
          &lt;li&gt;composing performance&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;audio generation : convert score with performance characteristics into audio
        &lt;ul&gt;
          &lt;li&gt;assign timbre&lt;/li&gt;
          &lt;li&gt;directry generate music in audio&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;그 중에서도 SCORE GENERATION이 가장 핫함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;style transfer 적용
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/3a730a16-e14a-4174-8d27-05807196346f/image.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;history&quot;&gt;History&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Iliac Suite : computer 로 만들어진 최초의 score
    &lt;ul&gt;
      &lt;li&gt;Markov chain 활용(Stochastic Model)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Coconet&lt;/strong&gt;
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/5b0df87e-0c0b-48b7-892a-b8958dd41752/image.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/e5825a0d-c3fd-450f-91f1-d034fe4c5c91/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;4마디 단위로 잘라서 멜로디에 alto,tenor,bass에 대응하는 멜로디 추가&lt;/li&gt;
      &lt;li&gt;piano roll type의 input을 one-hot vector로 encoding&lt;/li&gt;
      &lt;li&gt;16분음표로 quantization&lt;/li&gt;
      &lt;li&gt;전형적인 딥러닝의 multiclassification task&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Todd’s &lt;strong&gt;Time-Windowed&lt;/strong&gt; and conditioned recurrent architectures
    &lt;ol&gt;
      &lt;li&gt;인공신경망을 어떻게 음악생성에 활용할 수 있을지를 첫번쨰로 보여준 사례
        &lt;ol&gt;
          &lt;li&gt;input : context와 plan으로 나뉨
            &lt;ul&gt;
              &lt;li&gt;context :memory
      - bos + 각 note에 부합하는 unit으로 구성&lt;/li&gt;
              &lt;li&gt;plan : network가 학습한 특정 melody&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;학습방법
 &lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/78d3fb95-60f1-4f67-9b53-088ade4f5879/image.png&quot; alt=&quot;&quot; /&gt;
 1) plan에서 melody 와
 2) 초기화된 context의 첫번쨰 시점으로부터
 3) 첫번쨰 output을 냄 (비교 및 weight update)
 4) output은 다음 time step의 memory로 들어감
 -&amp;gt; iterative하게 output을 기억하여 생성!&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Lewis Creation by refinement&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;traditional-music-generation&quot;&gt;Traditional Music Generation&lt;/h3&gt;
&lt;p&gt;1) rule-based
    - linguistic &amp;amp; grammar 활용
    - 음악마다 다른 rule 이 만들어져야 하는 단점
    - 
2) Probability model&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;semi-automatic 한 알고리즘에 Markov chain이나 이외의 기술을 태움&lt;/li&gt;
  &lt;li&gt;원래의 data로부터 subsequence를 생성하지만, memory가 없어서 input에 따라 조건부 확률이 변동이 심함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3) Neural Network(RNN)
    - RNN으로 feature를 학습하여 단선율의 pitch와 duration을 학습
    - Gradient Vanishing으로 long-term dependency 학습 못하는 단점&lt;/p&gt;

&lt;h3 id=&quot;evloutionary&quot;&gt;evloutionary&lt;/h3&gt;
&lt;p&gt;1) GenJam
2) Director Musices
3) Probablistic&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;hierarchical HMM&lt;/li&gt;
  &lt;li&gt;dynamic Bayesian networks&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sound-modelling&quot;&gt;sound modelling&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;physical modeling&lt;/li&gt;
  &lt;li&gt;acoustic modeling : signal generator 이용
    &lt;ul&gt;
      &lt;li&gt;oscillator, modulator, filters와 같이 waveform 조작을 위한 processors&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lyrics&quot;&gt;lyrics&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;가사 포함 여부에 따라 music audio를 audio와 singing voice로 나눔.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-music-generation&quot;&gt;Deep Music Generation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Google Magenta : &lt;a href=&quot;https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn&quot;&gt;Melody RNN model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Anticipation-RNN : 사용자가 정의한 positional한 제약을 강요함.&lt;/li&gt;
  &lt;li&gt;TP-LSTM-NADE and BALSTM : 병렬 RNN으로부터 다선율의 데이터셋에서 translation-invariance를 유지&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;score-generation-vae-gan-transformer-&quot;&gt;Score Generation: VAE, GAN, Transformer, …&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;MusicVAE : longterm dependency, interpolation &amp;amp; reconstruction 퍼포먼스 굳
    &lt;ul&gt;
      &lt;li&gt;coupled latent variable model&lt;/li&gt;
      &lt;li&gt;binary regularizer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GAN
    &lt;ul&gt;
      &lt;li&gt;sequential data에는 학습에 어려움이 있음&lt;/li&gt;
      &lt;li&gt;CNN based GAN으로 개발&lt;/li&gt;
      &lt;li&gt;GAN-based MidiNet : Chord 에 condition되어 마디를 생성하는 알고리즘&lt;/li&gt;
      &lt;li&gt;MuseGAN : multi-track 다선율 음악을 생성하는 최초의 모델
        &lt;ul&gt;
          &lt;li&gt;강화학습을 접목하여 RNN-based GAN 적용&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transformer
    &lt;ul&gt;
      &lt;li&gt;REMI&lt;/li&gt;
      &lt;li&gt;Transformer XL&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;performance-generation--대부분-rnn&quot;&gt;Performance Generation : 대부분 RNN&lt;/h2&gt;

&lt;h2 id=&quot;representation&quot;&gt;Representation&lt;/h2&gt;
&lt;p&gt;1) symbol domain&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;discrete variable
2) audio domain&lt;/li&gt;
  &lt;li&gt;continuous variable&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;From artificial neural networks to deep learning for music generation: history, concepts and trends&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/generating-music-with-artificial-intelligence-9ce3c9eef806&lt;/li&gt;
  &lt;li&gt;https://topten.ai/music-generators-review/&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="nlp" />
      

      
        <summary type="html">Abstract 딥러닝을 활용한 최근의 다양한 음악 작곡 TASK를 소개한 논문 DATASETS MUSIC REPRESENTATION EVALUATION METHOD challenges &amp;amp; future directions</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Music Generation and AI, present and future</title>
      <link href="https://crosstar1228.github.io/NLP-music_and_ai" rel="alternate" type="text/html" title="Music Generation and AI, present and future" />
      <published>2022-07-18T11:00:00+00:00</published>
      <updated>2022-07-18T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/NLP-music_and_ai</id>
      <content type="html" xml:base="https://crosstar1228.github.io/NLP-music_and_ai">&lt;h2 id=&quot;목차&quot;&gt;목차&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://velog.io/@tobigsvoice1516/5%EC%A3%BC%EC%B0%A8-MUSIC-COMPOSITION-WITH-DEEP-LEARNING-A-REVIEW&quot;&gt;music &amp;amp; ai 역사&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openai.com/blog/jukebox/&quot;&gt;OpenAI - JukeBox&lt;/a&gt; [&lt;a href=&quot;https://github.com/openai/jukebox/&quot;&gt;Github&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;Datasets&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ai-music-genration의-시초&quot;&gt;AI Music Genration의 시초&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;90년대 David Bowie 의 Verbasizer (앱)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;단어를 임의로 재배치하여 음악 가사에 사용될 수 있도록 재조합하는 앱이었음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;2016년 Sony의 App Flow Machine
    &lt;ul&gt;
      &lt;li&gt;비틀즈 스타일 멜로디를 창조해 냄&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;music-generation-도-크게-다르지-않아&quot;&gt;Music Generation 도 크게 다르지 않아&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;머신러닝에서 모델은 다량의 데이터를 학습하고 그 안에서 ‘패턴’을 찾아냅니다.&lt;/li&gt;
  &lt;li&gt;Music Generation에서는 그 패턴이 Chord, Tempo, lengths, note 간 관계성 등 이됩니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;symbolic-approach-non-symbolic-approach&quot;&gt;Symbolic approach, Non-symbolic approach&lt;/h3&gt;

&lt;h2 id=&quot;music-generation의-고질적인-문제-1--long-term-dependency&quot;&gt;Music Generation의 고질적인 문제 1 : LONG TERM DEPENDENCY&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;해결법 1 : autoencoder로 저차원 space로 mapping
    &lt;ul&gt;
      &lt;li&gt;불필요한 정보를 버리게 됨&lt;/li&gt;
      &lt;li&gt;이후 upsampling&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MuseNet : midi data 기반 많은 양의 데이터 학습&lt;/li&gt;
  &lt;li&gt;Transfomer 계열 모델로 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;아이디어-&quot;&gt;아이디어 :&lt;/h3&gt;
&lt;p&gt;o learn a lower-dimensional encoding of the audio with the goal of losing the less important
information but retaining most of the musical information&lt;/p&gt;
&lt;h2 id=&quot;문제-2--diversityvariation&quot;&gt;문제 2 : Diversity(variation)&lt;/h2&gt;

&lt;h3 id=&quot;jukeboxpaper&quot;&gt;JukeBox[&lt;a href=&quot;https://arxiv.org/abs/2005.00341&quot;&gt;Paper&lt;/a&gt;]&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;long context 를 autoregressiveTransformer 이용한 multi-sclae VQ-VAE로 해결&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lyric-conditioning&quot;&gt;Lyric Conditioning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;노래의 duration에 linear 하게 가사의 문자들을 align하는 방법&lt;/li&gt;
  &lt;li&gt;가사를 위한 encoder를 더하고, &lt;strong&gt;music decoder로부터 의 query&lt;/strong&gt;로부터 &lt;strong&gt;가사 encoder로부터의 key, value 쌍&lt;/strong&gt; 으로의 attetion layer를 적용함.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vq-vae-codebook-collapse&quot;&gt;VQ-VAE codebook collapse&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;codebook에 mapping된 embedding vector들이 많이 쓰이지 않는 현상&lt;/li&gt;
  &lt;li&gt;Random Restart:codebook vector 사용량이 평균이하로 떨어지면 , encoder output 중 하나로 다시 reset&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;https://magenta.tensorflow.org/perceiver-ar&lt;/p&gt;

&lt;h3 id=&quot;sparse-transformer&quot;&gt;Sparse Transformer&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;sparsifies the attention pattern by
reshaping the input sequence into a &lt;strong&gt;2-D sequence&lt;/strong&gt; of
shape&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;google-deepmind-202206&quot;&gt;Google Deepmind (2022.06)&lt;/h2&gt;
&lt;p&gt;[&lt;a href=&quot;https://arxiv.org/abs/2202.07765&quot;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;h3 id=&quot;perceiver-ar&quot;&gt;Perceiver AR&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;modality 에 대하여 agnostic(인지불능)인 구조
    &lt;ul&gt;
      &lt;li&gt;cross attention : long-range input -&amp;gt; small latent&lt;/li&gt;
      &lt;li&gt;maintaining end-to-end causal masking
https://soundraw.io/ 
https://magenta.tensorflow.org/
https://www.aiva.ai/
-&amp;gt; 음악 작곡&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Datasets&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://paperswithcode.com/task/music-generation&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;music-generation-and-deep-learning&quot;&gt;Music Generation and Deep learning&lt;/h2&gt;
&lt;p&gt;1) 딥러닝 베이스 음악 생성의 컨셉
2) 음악 생성의 다양한 방법과 원리
3) 다양한 음악 생성의 개념적 분류 체계
4) 트렌드&lt;/p&gt;

&lt;p&gt;abstract model이 generation을 위해 사용됨&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sota models
MuseGAN
Melnet
MidiNet&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;From artificial neural networks to deep learning for music generation: history, concepts and trends&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/generating-music-with-artificial-intelligence-9ce3c9eef806&lt;/li&gt;
  &lt;li&gt;https://topten.ai/music-generators-review/&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="nlp" />
      

      
        <summary type="html">목차 music &amp;amp; ai 역사 OpenAI - JukeBox [Github] Datasets</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Machine learning (1) Generative Model과 GAN</title>
      <link href="https://crosstar1228.github.io/ML-GAN2" rel="alternate" type="text/html" title="Machine learning (1) Generative Model과 GAN" />
      <published>2021-09-15T11:00:00+00:00</published>
      <updated>2021-09-15T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/ML-GAN2</id>
      <content type="html" xml:base="https://crosstar1228.github.io/ML-GAN2">&lt;h1 id=&quot;안녕&quot;&gt;안녕&lt;/h1&gt;
&lt;p&gt;머신러닝 잘하고싶다.~~~~~~~~~~!!!!!!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;진짜로.&lt;/strong&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;후&lt;/code&gt;…&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="nlp" />
      

      
        <summary type="html">안녕 머신러닝 잘하고싶다.~~~~~~~~~~!!!!!!</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Machine learning (1) Generative Model과 GAN</title>
      <link href="https://crosstar1228.github.io/ML-GAN" rel="alternate" type="text/html" title="Machine learning (1) Generative Model과 GAN" />
      <published>2021-09-15T11:00:00+00:00</published>
      <updated>2021-09-15T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/ML-GAN</id>
      <content type="html" xml:base="https://crosstar1228.github.io/ML-GAN">&lt;h1 id=&quot;안녕&quot;&gt;안녕&lt;/h1&gt;
&lt;p&gt;머신러닝 잘하고싶다.~~~~~~~~~~!!!!!!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;진짜로.&lt;/strong&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;후&lt;/code&gt;…&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="nlp" />
      

      
        <summary type="html">안녕 머신러닝 잘하고싶다.~~~~~~~~~~!!!!!!</summary>
      

      
      
    </entry>
  
</feed>
