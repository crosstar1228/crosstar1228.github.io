<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://crosstar1228.github.io/tag/nlp/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://crosstar1228.github.io/" rel="alternate" type="text/html" />
  <updated>2022-10-29T11:51:18+00:00</updated>
  <id>https://crosstar1228.github.io/tag/nlp/feed.xml</id>

  
  
  

  
    <title type="html">건너별의 Romantic AI | </title>
  

  
    <subtitle>IT/인공지능 서랍장</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">Symbolic Music Representation - 1D, 2D Future Directions</title>
      <link href="https://crosstar1228.github.io/NLP-music_generation_input_output" rel="alternate" type="text/html" title="Symbolic Music Representation - 1D, 2D Future Directions" />
      <published>2022-09-13T11:00:00+00:00</published>
      <updated>2022-09-13T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/NLP-music_generation_input_output</id>
      <content type="html" xml:base="https://crosstar1228.github.io/NLP-music_generation_input_output">&lt;blockquote&gt;
  &lt;p&gt;Music representation은 크게 Symbol domain 과 audio domain으로 나뉩니다. 그 중 midi 파일의 표현 방식이자, discrete한 representation을 가지는
&lt;strong&gt;Symbolic music generation&lt;/strong&gt;에 대해 알아봅시다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;symbolic-music-domain&quot;&gt;Symbolic Music Domain&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;discrete 하고 표현이 명확함&lt;/li&gt;
  &lt;li&gt;dynamic 과 같은 악보 외적인 사항을 학습할 수 있음&lt;/li&gt;
  &lt;li&gt;특정악기에 customizing되어 있어서, 새로운 악기에 기존 modeling technique를 적용하는 것이 어려움&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1d-representation&quot;&gt;1D representation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;일반적인 표현 방법&lt;/li&gt;
  &lt;li&gt;note 간 관계 표현이 어려움(이전 note가 다음 note 시작까지 지속된다 등의 정보)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1-event-based&quot;&gt;1. Event-Based&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Midi-like event
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/063a5409-734b-4782-bde0-e7d71b66c7bb/image.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-sequence-based&quot;&gt;2. Sequence-Based&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;pitch, duration, chord 와 다른 음악 정보를 여러 list형태로 표현&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2d-representation&quot;&gt;2D representation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;sampling time에 의해 note를 2D matrix로 표현&lt;/li&gt;
  &lt;li&gt;더 높은 resolution을 필요로 함
    &lt;ul&gt;
      &lt;li&gt;rhythm이 복잡해지는 등 time dimension이 높아지면 장기적 의존관계를 학습하기 어려움
        &lt;h3 id=&quot;piano-roll&quot;&gt;Piano Roll&lt;/h3&gt;
        &lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/3856593a-eecb-410a-9ead-ff81f5893df4/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Automatic Piano에 의해 영감을 받음&lt;/li&gt;
  &lt;li&gt;(pitch 수 + resting state + holding state) * (resolution)모양
    &lt;ul&gt;
      &lt;li&gt;높이는 일반적으로 130개&lt;/li&gt;
      &lt;li&gt;너비는 일반적으로&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;하나의 matrix는 하나의 track을 의미하며, 여러 개의 track은 여러 matrix로 표현됨&lt;/li&gt;
  &lt;li&gt;pretty_midi 또는 Pypianoroll python toolkit이용하여 손쉽게 변환 가능&lt;/li&gt;
  &lt;li&gt;일반적으로 binary
    &lt;ul&gt;
      &lt;li&gt;각 position은 note가 그 position에서 연주되고 있는지 아닌지를 표현&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;note-off 정보가 없어서 16분음표 2개 반복과, 8분음표 하나를 같게 인식함&lt;/li&gt;
  &lt;li&gt;이를 개선하는 방법에는
    &lt;ul&gt;
      &lt;li&gt;hold replay&lt;/li&gt;
      &lt;li&gt;time step을 note beginning과 note end로 나눔&lt;/li&gt;
      &lt;li&gt;hold 를 표현하는 symbol인 ‘_’ 를 사용(monophonic melody에만 가능)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Conlon pianoroll : CNN에서 receptive field가 크지 않아도 되는 장점&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;A Comprehensive Survey on Deep Music Generation(2020)&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="nlp" />
      

      
        <summary type="html">Music representation은 크게 Symbol domain 과 audio domain으로 나뉩니다. 그 중 midi 파일의 표현 방식이자, discrete한 representation을 가지는 Symbolic music generation에 대해 알아봅시다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">(Paper Review) AudioLM ; a language Modeling Approach to Audio Generation</title>
      <link href="https://crosstar1228.github.io/NLP-AudioLM" rel="alternate" type="text/html" title="(Paper Review) AudioLM ; a language Modeling Approach to Audio Generation" />
      <published>2022-08-27T11:00:00+00:00</published>
      <updated>2022-08-27T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/NLP-AudioLM</id>
      <content type="html" xml:base="https://crosstar1228.github.io/NLP-AudioLM">&lt;h2 id=&quot;논문-간단-소개&quot;&gt;논문 간단 소개&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;Google Research 에서 개발한 모델. Semantic 과 acuostic, 두 갈래의 tokenization으로 나누어 성능 향샹을 이룸&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Transcript(대본)을 통한 audio generation process가 주를 이루어왔음
    &lt;ul&gt;
      &lt;li&gt;speech synthesis에서 텍스트 대본&lt;/li&gt;
      &lt;li&gt;피아노에서 midi Representation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;하지만 transcript가 없는 상황에서는 생성에 어려움을 겪음
    &lt;ul&gt;
      &lt;li&gt;speech voice recover : speaker characteristic 이 필요함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;auodiolm의-허들&quot;&gt;AuodioLM의 허들&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;대규모 말뭉치를 학습한 language model들은 최근 좋은 성능을 보임&lt;/li&gt;
  &lt;li&gt;AudioLM은 그러한 진보에 박차를 가하여(그 구조를 따와서) &lt;strong&gt;annotated data 없이 audio를 생성할 수 있음&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;하지만 language model로부터 audio language model로 가려면 허들이 존재함
    &lt;ol&gt;
      &lt;li&gt;audio 생성을 위해서는 language 생성보다 더 밀도있는 데이터가 필요함&lt;/li&gt;
      &lt;li&gt;text와 audio는 one-to-many 관계에 있어서, 같은 문장도 speaker, 감정, 스타일에 따라 달라짐&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;이러한-허들을-극복하기-위하여&quot;&gt;이러한 허들을 극복하기 위하여&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/71e3f8d5-0325-4df0-a867-096b632377c1/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;semantic token (from &lt;a href=&quot;https://deepai.org/publication/w2v-bert-combining-contrastive-learning-and-masked-language-modeling-for-self-supervised-speech-pre-training&quot;&gt;W2v-BERT&lt;/a&gt;)
    &lt;ul&gt;
      &lt;li&gt;local dependency(speech의 음소, 음악의 melody)&lt;/li&gt;
      &lt;li&gt;global long-term structure(speech의 맥락, 음악의 harmony나 rhythm)&lt;/li&gt;
      &lt;li&gt;긴 sequence를 허락하기 위하여 audio signal을 downsampling함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;acoustic token (from &lt;a href=&quot;https://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html&quot;&gt;SoundStream neural codec&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;audio-only-language-model&quot;&gt;Audio-Only language model&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;AudioLM은 audio data만을 학습(text나 symbolic representation을 학습하지 않음)&lt;/li&gt;
  &lt;li&gt;Hierachical한 모델(high level-&amp;gt; low level)
    &lt;ul&gt;
      &lt;li&gt;semantic token -&amp;gt; coarse acoustic token -&amp;gt; fine acoustic token으로 chainin하는 구조&lt;/li&gt;
      &lt;li&gt;chain 사이사이에 transformer 모델이 활용됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1-semantic-token&quot;&gt;1. Semantic token&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/8814e6d5-706b-4fd9-8683-85bba144938e/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;language model과 같이 next token을 예측하는 구조&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-coarse-acoustic-token&quot;&gt;2. coarse acoustic token&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/a915f356-ab6e-46ba-8c9d-b3114e45318f/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Genrerated token 과 poast coarse acoustic token을 concat함&lt;/li&gt;
  &lt;li&gt;이러한 형태로 acoustic token이 예측됨&lt;/li&gt;
  &lt;li&gt;이 단계에서 speaker의 특성(음색 등)이 모델링됨&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3-fine-acoustic-token&quot;&gt;3. fine acoustic token&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/329274e9-1577-4224-b561-e2b18b4b269b/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;최종 audio에 detail을 추가함&lt;/li&gt;
  &lt;li&gt;최종 acoustic token을 Soundstream decoder를 통해 reconstruct&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;결론적으로&quot;&gt;결론적으로&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;piano continumation ,speech continuation, unconditional generation 등의 task에서 좋은 성능을 냄&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://google-research.github.io/seanet/audiolm/examples/&quot;&gt;이 페이지&lt;/a&gt;에서 디테일한 확인이 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="nlp" />
      

      
        <summary type="html">논문 간단 소개 Google Research 에서 개발한 모델. Semantic 과 acuostic, 두 갈래의 tokenization으로 나누어 성능 향샹을 이룸</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">\[Paper Review\] Meshed-memory transformer for image captioning</title>
      <link href="https://crosstar1228.github.io/NLP-meshed_memory_transformer" rel="alternate" type="text/html" title="\[Paper Review\] Meshed-memory transformer for image captioning" />
      <published>2022-08-27T11:00:00+00:00</published>
      <updated>2022-08-27T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/NLP-meshed_memory_transformer</id>
      <content type="html" xml:base="https://crosstar1228.github.io/NLP-meshed_memory_transformer">&lt;h2 id=&quot;논문-간단-소개&quot;&gt;논문 간단 소개&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;image captioning task에서 transformer 모델을 활용한 모델 중 가장
    &lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
  &lt;/li&gt;
  &lt;li&gt;image encoding
    &lt;ul&gt;
      &lt;li&gt;학습된 사전 지식(caption)을 기반으로 image region간의 multi-level representation을 학습&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;language generation
    &lt;ul&gt;
      &lt;li&gt;low-level과 high-level feature를 모두 활용하는 mesh-like connectivity 활용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$M^2$ Transformer
    &lt;ul&gt;
      &lt;li&gt;다른 fully-attentive model과 비교해서 성능을 비교하고,&lt;/li&gt;
      &lt;li&gt;COCO image-text set에서 sota 기록&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;RNN + attention, Transformer, Bert 등 다양하게 이용되어 왔음&lt;/li&gt;
  &lt;li&gt;multimodal task는 기존 unimodal task와 다른 구조를 띠어야 함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/b1a321d2-0d39-49d6-881d-9eb273f3e5b6/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;encoding&quot;&gt;encoding&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MemoryAugmentedEncoding&lt;/code&gt;
    &lt;ol&gt;
      &lt;li&gt;이미지 구역 및 그 사이 관계가 multi-level(low-level, high-level) 로 인코딩됨&lt;/li&gt;
      &lt;li&gt;memory vector를 사용하여 사전지식을 encoding 및  관계를 모델링함&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;decoding&quot;&gt;decoding&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MeshedDecoding&lt;/code&gt;
    &lt;ol&gt;
      &lt;li&gt;학습된 gated mechanism으로 달성되는데, 각 단계별로 multi-level 의 기여도를 weight화함&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;→ encoder 와 decoder 간에 meshed connectivity 로 구조화됨&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;structure&quot;&gt;Structure&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/4f30ef85-a228-47d9-bac3-9a23b8781ddc/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;encoding-1&quot;&gt;Encoding&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;input image로부터 추출된 image region X (집합)&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  - 현재 image detection dataset을 활용하지만, 마디별 note 및 chord 범위로 설정 예정
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/49a747fe-4d6f-4383-ac83-2e9d820d7cd5/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;learnable weights W(q,k,v 에 대응되는)&lt;/li&gt;
  &lt;li&gt;S(X) = X의 weighted sum of values&lt;/li&gt;
  &lt;li&gt;image feature(region) 간의 pairwise similarity
    &lt;ul&gt;
      &lt;li&gt;하지만 이러한 방식의 self-attention은 사전 지식( a priori knowlendge)를 반영하지 못함&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;ex) 사람과 농구공이 있으면 player나 game과 같은 정보를 추론해내기 어려움&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1-memory-augmented-attention&quot;&gt;1. Memory-Augmented Attention&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;memory로 data augmentation이 진행되었다.&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;key 또는 value가 slot의 형태로 확장되고, 이는 사전지식을 encoding할 수 있음
      &lt;ul&gt;
        &lt;li&gt;이러한 slot은 learnable vector로서 SGD로 업데이트 가능(parameter)&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/fa37f3f2-0f75-46c7-9d13-ef4b2b917352/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$M_k, M_v$ 는 $n_m$ 개의 row수를 가진 학습가능한 matrix&lt;/li&gt;
  &lt;li&gt;, 는 concatenation을 의미&lt;/li&gt;
  &lt;li&gt;위 learnable parameter에 의해 X에 embedded 되지 않은 정보를 학습 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-encoding-layer&quot;&gt;2. Encoding layer&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/61222ccd-f0c5-41d8-84e3-11eb53d56b78/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이후 Memory augmented attention이 Position-Wise FFL 에 적용
    &lt;ul&gt;
      &lt;li&gt;2개의 affine transformation으로 이루어짐(non-linearity는 한곳에만 적용)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3-residual-connection--layer-norm&quot;&gt;3. Residual Connection + layer norm&lt;/h3&gt;

&lt;p&gt;각각의 sub-component(Memory-augmented attention 과 Encoding Layer)가 위 방식으로 감싸짐&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/34fe79cd-e000-41d1-acb7-40edc9bc5d97/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;AddNorm은 Residual Connection + Layer Normalization&lt;/p&gt;

&lt;h3 id=&quot;4-full-encoder&quot;&gt;4. Full encoder&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;여러 layer, 이전 레이어 아웃풋이 다음 레이어 인풋으로 들어감&lt;/li&gt;
  &lt;li&gt;더 높은 encoding layer는 이전에 이미 인식된 관계 정보를 사용할 수 있음&lt;/li&gt;
  &lt;li&gt;다양한 수준의 output을 생성&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;decoding-1&quot;&gt;Decoding&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/4f30ef85-a228-47d9-bac3-9a23b8781ddc/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;32-meshed-decoder&quot;&gt;3.2 Meshed Decoder&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;이전 step의 생성된 word와 region encoding&lt;/li&gt;
  &lt;li&gt;multi-layer structure&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;meshed-cross-attention&quot;&gt;meshed Cross attention&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;transformer의 cross-attention과 달리, 모든 encoding layer를 활용하여 생성 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/44e2c41e-8cde-4d63-9bd3-874033402a97/image.png&quot; alt=&quot;&quot; /&gt;
$C$ : encoder-decoder cross attention&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;decoder 로부터의 query와, encoder로부터의 key-value 쌍의 cross attention&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/cc5cd9d8-34e0-4e3b-bf96-1e2c382cbcaf/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\alpha_i$ : cross-attention 결과와 같은 크기의 weight matrix&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;각 encoding layer의 기여도 및 상대적인 중요도를 조절해주는 값&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/5cbb125d-5ded-454a-b644-3878ff311e37/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;input query와 Cross Attention output간의 관련성 측정한 결과값&lt;/li&gt;
  &lt;li&gt;$W_i$ 는 2d*d matrix&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;masked-self-attention&quot;&gt;Masked Self attention&lt;/h3&gt;

&lt;p&gt;$S_mask$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;input sequence Y의 t번째 element로부터의 query&lt;/li&gt;
  &lt;li&gt;왼쪽의 subsequence로부터 얻은 key와 value&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ffw--addnorm&quot;&gt;FFW + AddNorm&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/b237802b-2b01-49fa-802d-2bc333b54d38/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;여러 layer로 stack&lt;/li&gt;
  &lt;li&gt;결과적으로 t 번째 element에 기반하여 t+1 번째 시점이 예측됨&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;XE(word-level Crossentropy loss)로 pre-training
    &lt;ul&gt;
      &lt;li&gt;
        &lt;h2 id=&quot;ground-truth-word를-기반으로-다음-token을-예측&quot;&gt;Ground-truth word를 기반으로 다음 token을 예측&lt;/h2&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reinforcement learning 활용한 예측
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;self-critical sequence training approach&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;topk&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataset--coco&quot;&gt;Dataset : COCO&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;120,000 image with 5 captions each&lt;/li&gt;
  &lt;li&gt;nocap : 15,100 images annotated with 11 human-generated captions&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;From artificial neural networks to deep learning for music generation: history, concepts and trends&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/generating-music-with-artificial-intelligence-9ce3c9eef806&lt;/li&gt;
  &lt;li&gt;https://topten.ai/music-generators-review/&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="nlp" />
      

      
        <summary type="html">논문 간단 소개 image captioning task에서 transformer 모델을 활용한 모델 중 가장 Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">A Comprehensive Survey on Deep Music Generation - Multi-level Representations, Algorithms, Evaluations, and Future Directions</title>
      <link href="https://crosstar1228.github.io/NLP-music_generation_paper_review_1" rel="alternate" type="text/html" title="A Comprehensive Survey on Deep Music Generation - Multi-level Representations, Algorithms, Evaluations, and Future Directions" />
      <published>2022-08-27T11:00:00+00:00</published>
      <updated>2022-08-27T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/NLP-music_generation_paper_review_1</id>
      <content type="html" xml:base="https://crosstar1228.github.io/NLP-music_generation_paper_review_1">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;딥러닝을 활용한 최근의 다양한 음악 작곡 TASK를 소개한 논문
    &lt;ul&gt;
      &lt;li&gt;DATASETS&lt;/li&gt;
      &lt;li&gt;MUSIC REPRESENTATION&lt;/li&gt;
      &lt;li&gt;EVALUATION METHOD&lt;/li&gt;
      &lt;li&gt;challenges &amp;amp; future directions&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;intro&quot;&gt;Intro&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;음악은 언어와 같다.&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;악보 : 음악  == 글 : 말&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Music Generation의 대분류 3개
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/33ed8e71-a0d8-4853-9c90-c9f1940900dc/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;score generation&lt;/li&gt;
      &lt;li&gt;performance generation add performance characteristics to scores
        &lt;ul&gt;
          &lt;li&gt;RENDERING performance&lt;/li&gt;
          &lt;li&gt;composing performance&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;audio generation : convert score with performance characteristics into audio
        &lt;ul&gt;
          &lt;li&gt;assign timbre&lt;/li&gt;
          &lt;li&gt;directry generate music in audio&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;그 중에서도 SCORE GENERATION이 가장 핫함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;style transfer 적용
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/3a730a16-e14a-4174-8d27-05807196346f/image.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;history&quot;&gt;History&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Iliac Suite : computer 로 만들어진 최초의 score
    &lt;ul&gt;
      &lt;li&gt;Markov chain 활용(Stochastic Model)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Coconet&lt;/strong&gt;
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/5b0df87e-0c0b-48b7-892a-b8958dd41752/image.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/e5825a0d-c3fd-450f-91f1-d034fe4c5c91/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;4마디 단위로 잘라서 멜로디에 alto,tenor,bass에 대응하는 멜로디 추가&lt;/li&gt;
      &lt;li&gt;piano roll type의 input을 one-hot vector로 encoding&lt;/li&gt;
      &lt;li&gt;16분음표로 quantization&lt;/li&gt;
      &lt;li&gt;전형적인 딥러닝의 multiclassification task&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Todd’s &lt;strong&gt;Time-Windowed&lt;/strong&gt; and conditioned recurrent architectures
    &lt;ol&gt;
      &lt;li&gt;인공신경망을 어떻게 음악생성에 활용할 수 있을지를 첫번쨰로 보여준 사례
        &lt;ol&gt;
          &lt;li&gt;input : context와 plan으로 나뉨
            &lt;ul&gt;
              &lt;li&gt;context :memory
      - bos + 각 note에 부합하는 unit으로 구성&lt;/li&gt;
              &lt;li&gt;plan : network가 학습한 특정 melody&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;학습방법
 &lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/78d3fb95-60f1-4f67-9b53-088ade4f5879/image.png&quot; alt=&quot;&quot; /&gt;
 1) plan에서 melody 와
 2) 초기화된 context의 첫번쨰 시점으로부터
 3) 첫번쨰 output을 냄 (비교 및 weight update)
 4) output은 다음 time step의 memory로 들어감
 -&amp;gt; iterative하게 output을 기억하여 생성!&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Lewis Creation by refinement&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;traditional-music-generation&quot;&gt;Traditional Music Generation&lt;/h3&gt;
&lt;p&gt;1) rule-based
    - linguistic &amp;amp; grammar 활용
    - 음악마다 다른 rule 이 만들어져야 하는 단점
    - 
2) Probability model&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;semi-automatic 한 알고리즘에 Markov chain이나 이외의 기술을 태움&lt;/li&gt;
  &lt;li&gt;원래의 data로부터 subsequence를 생성하지만, memory가 없어서 input에 따라 조건부 확률이 변동이 심함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3) Neural Network(RNN)
    - RNN으로 feature를 학습하여 단선율의 pitch와 duration을 학습
    - Gradient Vanishing으로 long-term dependency 학습 못하는 단점&lt;/p&gt;

&lt;h3 id=&quot;evloutionary&quot;&gt;evloutionary&lt;/h3&gt;
&lt;p&gt;1) GenJam
2) Director Musices
3) Probablistic&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;hierarchical HMM&lt;/li&gt;
  &lt;li&gt;dynamic Bayesian networks&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sound-modelling&quot;&gt;sound modelling&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;physical modeling&lt;/li&gt;
  &lt;li&gt;acoustic modeling : signal generator 이용
    &lt;ul&gt;
      &lt;li&gt;oscillator, modulator, filters와 같이 waveform 조작을 위한 processors&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lyrics&quot;&gt;lyrics&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;가사 포함 여부에 따라 music audio를 audio와 singing voice로 나눔.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-music-generation&quot;&gt;Deep Music Generation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Google Magenta : &lt;a href=&quot;https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn&quot;&gt;Melody RNN model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Anticipation-RNN : 사용자가 정의한 positional한 제약을 강요함.&lt;/li&gt;
  &lt;li&gt;TP-LSTM-NADE and BALSTM : 병렬 RNN으로부터 다선율의 데이터셋에서 translation-invariance를 유지&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;score-generation-vae-gan-transformer-&quot;&gt;Score Generation: VAE, GAN, Transformer, …&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;MusicVAE : longterm dependency, interpolation &amp;amp; reconstruction 퍼포먼스 굳
    &lt;ul&gt;
      &lt;li&gt;coupled latent variable model&lt;/li&gt;
      &lt;li&gt;binary regularizer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GAN
    &lt;ul&gt;
      &lt;li&gt;sequential data에는 학습에 어려움이 있음&lt;/li&gt;
      &lt;li&gt;CNN based GAN으로 개발&lt;/li&gt;
      &lt;li&gt;GAN-based MidiNet : Chord 에 condition되어 마디를 생성하는 알고리즘&lt;/li&gt;
      &lt;li&gt;MuseGAN : multi-track 다선율 음악을 생성하는 최초의 모델
        &lt;ul&gt;
          &lt;li&gt;강화학습을 접목하여 RNN-based GAN 적용&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transformer
    &lt;ul&gt;
      &lt;li&gt;REMI&lt;/li&gt;
      &lt;li&gt;Transformer XL&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;performance-generation--대부분-rnn&quot;&gt;Performance Generation : 대부분 RNN&lt;/h2&gt;

&lt;h2 id=&quot;representation&quot;&gt;Representation&lt;/h2&gt;
&lt;p&gt;1) symbol domain&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;discrete variable
2) audio domain&lt;/li&gt;
  &lt;li&gt;continuous variable&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;From artificial neural networks to deep learning for music generation: history, concepts and trends&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/generating-music-with-artificial-intelligence-9ce3c9eef806&lt;/li&gt;
  &lt;li&gt;https://topten.ai/music-generators-review/&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="nlp" />
      

      
        <summary type="html">Abstract 딥러닝을 활용한 최근의 다양한 음악 작곡 TASK를 소개한 논문 DATASETS MUSIC REPRESENTATION EVALUATION METHOD challenges &amp;amp; future directions</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Music Generation and AI, present and future</title>
      <link href="https://crosstar1228.github.io/NLP-music_and_ai" rel="alternate" type="text/html" title="Music Generation and AI, present and future" />
      <published>2022-07-18T11:00:00+00:00</published>
      <updated>2022-07-18T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/NLP-music_and_ai</id>
      <content type="html" xml:base="https://crosstar1228.github.io/NLP-music_and_ai">&lt;h2 id=&quot;목차&quot;&gt;목차&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://velog.io/@tobigsvoice1516/5%EC%A3%BC%EC%B0%A8-MUSIC-COMPOSITION-WITH-DEEP-LEARNING-A-REVIEW&quot;&gt;music &amp;amp; ai 역사&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openai.com/blog/jukebox/&quot;&gt;OpenAI - JukeBox&lt;/a&gt; [&lt;a href=&quot;https://github.com/openai/jukebox/&quot;&gt;Github&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;Datasets&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ai-music-genration의-시초&quot;&gt;AI Music Genration의 시초&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;90년대 David Bowie 의 Verbasizer (앱)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;단어를 임의로 재배치하여 음악 가사에 사용될 수 있도록 재조합하는 앱이었음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;2016년 Sony의 App Flow Machine
    &lt;ul&gt;
      &lt;li&gt;비틀즈 스타일 멜로디를 창조해 냄&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;music-generation-도-크게-다르지-않아&quot;&gt;Music Generation 도 크게 다르지 않아&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;머신러닝에서 모델은 다량의 데이터를 학습하고 그 안에서 ‘패턴’을 찾아냅니다.&lt;/li&gt;
  &lt;li&gt;Music Generation에서는 그 패턴이 Chord, Tempo, lengths, note 간 관계성 등 이됩니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;symbolic-approach-non-symbolic-approach&quot;&gt;Symbolic approach, Non-symbolic approach&lt;/h3&gt;

&lt;h2 id=&quot;music-generation의-고질적인-문제-1--long-term-dependency&quot;&gt;Music Generation의 고질적인 문제 1 : LONG TERM DEPENDENCY&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;해결법 1 : autoencoder로 저차원 space로 mapping
    &lt;ul&gt;
      &lt;li&gt;불필요한 정보를 버리게 됨&lt;/li&gt;
      &lt;li&gt;이후 upsampling&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MuseNet : midi data 기반 많은 양의 데이터 학습&lt;/li&gt;
  &lt;li&gt;Transfomer 계열 모델로 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;아이디어-&quot;&gt;아이디어 :&lt;/h3&gt;
&lt;p&gt;o learn a lower-dimensional encoding of the audio with the goal of losing the less important
information but retaining most of the musical information&lt;/p&gt;
&lt;h2 id=&quot;문제-2--diversityvariation&quot;&gt;문제 2 : Diversity(variation)&lt;/h2&gt;

&lt;h3 id=&quot;jukeboxpaper&quot;&gt;JukeBox[&lt;a href=&quot;https://arxiv.org/abs/2005.00341&quot;&gt;Paper&lt;/a&gt;]&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;long context 를 autoregressiveTransformer 이용한 multi-sclae VQ-VAE로 해결&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lyric-conditioning&quot;&gt;Lyric Conditioning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;노래의 duration에 linear 하게 가사의 문자들을 align하는 방법&lt;/li&gt;
  &lt;li&gt;가사를 위한 encoder를 더하고, &lt;strong&gt;music decoder로부터 의 query&lt;/strong&gt;로부터 &lt;strong&gt;가사 encoder로부터의 key, value 쌍&lt;/strong&gt; 으로의 attetion layer를 적용함.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vq-vae-codebook-collapse&quot;&gt;VQ-VAE codebook collapse&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;codebook에 mapping된 embedding vector들이 많이 쓰이지 않는 현상&lt;/li&gt;
  &lt;li&gt;Random Restart:codebook vector 사용량이 평균이하로 떨어지면 , encoder output 중 하나로 다시 reset&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;https://magenta.tensorflow.org/perceiver-ar&lt;/p&gt;

&lt;h3 id=&quot;sparse-transformer&quot;&gt;Sparse Transformer&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;sparsifies the attention pattern by
reshaping the input sequence into a &lt;strong&gt;2-D sequence&lt;/strong&gt; of
shape&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;google-deepmind-202206&quot;&gt;Google Deepmind (2022.06)&lt;/h2&gt;
&lt;p&gt;[&lt;a href=&quot;https://arxiv.org/abs/2202.07765&quot;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;h3 id=&quot;perceiver-ar&quot;&gt;Perceiver AR&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;modality 에 대하여 agnostic(인지불능)인 구조
    &lt;ul&gt;
      &lt;li&gt;cross attention : long-range input -&amp;gt; small latent&lt;/li&gt;
      &lt;li&gt;maintaining end-to-end causal masking
https://soundraw.io/ 
https://magenta.tensorflow.org/
https://www.aiva.ai/
-&amp;gt; 음악 작곡&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Datasets&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://paperswithcode.com/task/music-generation&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;music-generation-and-deep-learning&quot;&gt;Music Generation and Deep learning&lt;/h2&gt;
&lt;p&gt;1) 딥러닝 베이스 음악 생성의 컨셉
2) 음악 생성의 다양한 방법과 원리
3) 다양한 음악 생성의 개념적 분류 체계
4) 트렌드&lt;/p&gt;

&lt;p&gt;abstract model이 generation을 위해 사용됨&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sota models
MuseGAN
Melnet
MidiNet&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;From artificial neural networks to deep learning for music generation: history, concepts and trends&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/generating-music-with-artificial-intelligence-9ce3c9eef806&lt;/li&gt;
  &lt;li&gt;https://topten.ai/music-generators-review/&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="nlp" />
      

      
        <summary type="html">목차 music &amp;amp; ai 역사 OpenAI - JukeBox [Github] Datasets</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Machine learning (1) Generative Model과 GAN</title>
      <link href="https://crosstar1228.github.io/ML-GAN2" rel="alternate" type="text/html" title="Machine learning (1) Generative Model과 GAN" />
      <published>2021-09-15T11:00:00+00:00</published>
      <updated>2021-09-15T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/ML-GAN2</id>
      <content type="html" xml:base="https://crosstar1228.github.io/ML-GAN2">&lt;h1 id=&quot;안녕&quot;&gt;안녕&lt;/h1&gt;
&lt;p&gt;머신러닝 잘하고싶다.~~~~~~~~~~!!!!!!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;진짜로.&lt;/strong&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;후&lt;/code&gt;…&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="nlp" />
      

      
        <summary type="html">안녕 머신러닝 잘하고싶다.~~~~~~~~~~!!!!!!</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Machine learning (1) Generative Model과 GAN</title>
      <link href="https://crosstar1228.github.io/ML-GAN" rel="alternate" type="text/html" title="Machine learning (1) Generative Model과 GAN" />
      <published>2021-09-15T11:00:00+00:00</published>
      <updated>2021-09-15T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/ML-GAN</id>
      <content type="html" xml:base="https://crosstar1228.github.io/ML-GAN">&lt;h1 id=&quot;안녕&quot;&gt;안녕&lt;/h1&gt;
&lt;p&gt;머신러닝 잘하고싶다.~~~~~~~~~~!!!!!!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;진짜로.&lt;/strong&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;후&lt;/code&gt;…&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="nlp" />
      

      
        <summary type="html">안녕 머신러닝 잘하고싶다.~~~~~~~~~~!!!!!!</summary>
      

      
      
    </entry>
  
</feed>
