<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://crosstar1228.github.io/author/crosstar/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://crosstar1228.github.io/" rel="alternate" type="text/html" />
  <updated>2022-10-24T14:08:38+00:00</updated>
  <id>https://crosstar1228.github.io/author/crosstar/feed.xml</id>

  
  
  

  
    <title type="html">건너별의 Romantic AI | </title>
  

  
    <subtitle>IT/인공지능 서랍장</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">(python중급) 깔끔하게 함수 리모델링해보자! itemgetter, attrgetter, methodcaller, partial</title>
      <link href="https://crosstar1228.github.io/python_operator" rel="alternate" type="text/html" title="(python중급) 깔끔하게 함수 리모델링해보자! itemgetter, attrgetter, methodcaller, partial" />
      <published>2022-10-13T11:00:00+00:00</published>
      <updated>2022-10-13T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/python_operator</id>
      <content type="html" xml:base="https://crosstar1228.github.io/python_operator">&lt;blockquote&gt;
  &lt;p&gt;Keyword : operator, itemmgetter, attrgetter, methodcaller&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;operator&quot;&gt;Operator&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;python operator 모듈 안에는 다양한 함수들이 있음&lt;/li&gt;
  &lt;li&gt;활용해서 가독성 있는 코딩이 가능함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;itemgetter&quot;&gt;itemgetter&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;index가 정해져있는 함수를 반환함
```
arr = [‘a’, ‘b’, ‘c’]
func = itemgetter(2)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;func(arr)&lt;/p&gt;
&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;blockquote&gt;
      &lt;p&gt;‘b’
```&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;attrgetter&quot;&gt;attrgetter&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;class의 attribute이름을 특정하면 그 attribute를 반환&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-angular2html&quot;&gt;f = attrgetter('name')
f(b)
&amp;gt;&amp;gt; b.name
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
  &lt;li&gt;여러개의 attribute나 중첩된 attribute 형태도 가능함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;methodcaller&quot;&gt;methodcaller&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;class의 method 특정해서 그 결과값을 반환&lt;/li&gt;
  &lt;li&gt;*args, **kwargs 등을 입력해서 해당 method input을 바로 입력하는 것도 가능!&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-angular2html&quot;&gt; f = methodcaller('name', 'foo', bar=1)
f(b) 
&amp;gt;&amp;gt;&amp;gt; b.name('foo', bar=1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;functoolspartial&quot;&gt;functools.partial&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;하나 이상의 인수가 채워진 또 하나의 함수를 만드는 데 사용&lt;/li&gt;
  &lt;li&gt;불필요한 반복작업 줄임&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# 첫 인자에 따라 더하기도 하고, 곱하기도 하는 함수
def add_mul(choice, *args):
    if choice == &quot;add&quot;:
        result = 0
        for i in args:
            result = result + i
    elif choice == &quot;mul&quot;:
        result = 1
        for i in args:
            result = result * i
    return result


# 더하는 함수를 만드는 일반적인 풀이 
def add(*args):
    return add_mul('add', *args)
    
# partial 사용할 경우 간결해진다! 
add = partial(add_mul, 'add')

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;회고&quot;&gt;회고&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;코드 가독성을 높이는 데 좋을 듯 하다.&lt;/li&gt;
  &lt;li&gt;다른 여러 사칙.논리 연산 등에도 유용해 보인다(공식문서 참고).&lt;/li&gt;
  &lt;li&gt;partial은 활용도가 높아 보인다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://docs.python.org/3/library/operator.html&lt;/li&gt;
  &lt;li&gt;https://wikidocs.net/109304&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="python" />
      

      
        <summary type="html">Keyword : operator, itemmgetter, attrgetter, methodcaller</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">python and database 1 - ORM and ODBC</title>
      <link href="https://crosstar1228.github.io/db1" rel="alternate" type="text/html" title="python and database 1 - ORM and ODBC" />
      <published>2022-10-10T11:00:00+00:00</published>
      <updated>2022-10-10T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/db1</id>
      <content type="html" xml:base="https://crosstar1228.github.io/db1">&lt;blockquote&gt;
  &lt;p&gt;Keyword : Transfer learning, Zero-shot learning&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/db_image.png&quot; alt=&quot;img.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;orm-vs-odbc&quot;&gt;ORM VS ODBC&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;ORM(Object Relational Mapping) : Python class를 이용하여 dB와 상호작용 가능&lt;/li&gt;
  &lt;li&gt;ODBC(Open Database Connectivity)
    &lt;ul&gt;
      &lt;li&gt;개발자가 원시 또는 기본 SQL 쿼리를 작성하여 DB와 직접 상호작용 가능&lt;/li&gt;
      &lt;li&gt;DB system(oracle, Mysql, …)에 관계없이 접근 가능&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;별도의-서버가-필요할까&quot;&gt;별도의 서버가 필요할까?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;메모리 이슈가 있는 것 같긴 한데 같은 서버에서 운용하는 것 자체가 쿤 문제는 아닌 것 같음&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pyodbc&quot;&gt;&lt;a href=&quot;https://mkleehammer.github.io/pyodbc/&quot;&gt;pyodbc&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;ODBC db를 간단하게 접근할 수 있는 open source python module&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-pyodbc-설치&quot;&gt;1. pyodbc 설치&lt;/h2&gt;

&lt;h2 id=&quot;orm-vs-odbc-1&quot;&gt;ORM VS ODBC&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ORM(Object Relational Mapping) : Python class를 이용하여 dB와 상호작용 가능&lt;/li&gt;
  &lt;li&gt;ODBC(Open Database Connectivity)
    &lt;ul&gt;
      &lt;li&gt;개발자가 원시 또는 기본 SQL 쿼리를 작성하여 DB와 직접 상호작용 가능&lt;/li&gt;
      &lt;li&gt;DB system(oracle, Mysql, …)에 관계없이 접근 가능&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;odbc-실습&quot;&gt;ODBC 실습&lt;/h1&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://learn.microsoft.com/ko-kr/sql/connect/python/pyodbc/python-sql-driver-pyodbc?view=sql-server-ver16&quot;&gt;Microsoft 공식 문서&lt;/a&gt;를 참고하였습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;2-설치-과정&quot;&gt;2. 설치 과정&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Microsoft ODBC 드라이버 설치&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;pyodbc 설치&lt;/p&gt;

    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pip&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyodbc&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://mindsdb.com/blog/self-service-machine-learning-with-intelligent-databases&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="backend" />
      

      
        <summary type="html">Keyword : Transfer learning, Zero-shot learning</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">What is zero-shot learning?</title>
      <link href="https://crosstar1228.github.io/zero_shot_learning" rel="alternate" type="text/html" title="What is zero-shot learning?" />
      <published>2022-09-30T11:00:00+00:00</published>
      <updated>2022-09-30T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/zero_shot_learning</id>
      <content type="html" xml:base="https://crosstar1228.github.io/zero_shot_learning">&lt;blockquote&gt;
  &lt;p&gt;Keyword : Transfer learning, Zero-shot learning&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/img.png&quot; alt=&quot;img.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;transfer-learning이란&quot;&gt;Transfer learning이란&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;머신러닝 모델이 특정 도메인에서 얻은 지식 및 정보를 다른 도메인(분야)에서 문제를 해결하는데 사용하는 방식&lt;/li&gt;
  &lt;li&gt;A라는 문제의 해답이 a면, 다른 도메인의 B라는 문제를 해결할 때 공통점을 분석해서 b라는 답을 풀어낼 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;zero-shot-learning이란&quot;&gt;Zero-shot learning이란&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Transfer-learning의 한 종류라고 할 수 있음&lt;/li&gt;
  &lt;li&gt;Training 과정에서 관측된 여러 데이터 class 외에, 새로은 class에 속하는 데이터를 스스로 학습하는 모델&lt;/li&gt;
  &lt;li&gt;한번도 본적 없기에 zero-shot이 됨&lt;/li&gt;
  &lt;li&gt;예를 들어, 말 데이터를 학습한 모델이 얼룩말 데이터에 대해 ‘얼룩말’이라고 답을 맞힐수 있는 기술&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;원리&quot;&gt;원리&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/built/images/zsl.png&quot; alt=&quot;img.png&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;이미지로부터 semantic feature, visual feature를 모두 추출하여 mapping function에 대응됨&lt;/li&gt;
  &lt;li&gt;이 mapping function은 관측된 class와 멀어진 정도를 측정하여, error값이 최소가 되는 값을 unseen class로 측정&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://en.wikipedia.org/wiki/Zero-shot_learning&lt;/li&gt;
  &lt;li&gt;https://medium.com/analytics-vidhya/zero-shot-action-recognition-in-videos-a-survey-793cdfafbf1b&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="ML" />
      

      
        <summary type="html">Keyword : Transfer learning, Zero-shot learning</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Symbolic Music Representation - 1D, 2D Future Directions</title>
      <link href="https://crosstar1228.github.io/NLP-music_generation_input_output" rel="alternate" type="text/html" title="Symbolic Music Representation - 1D, 2D Future Directions" />
      <published>2022-09-13T11:00:00+00:00</published>
      <updated>2022-09-13T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/NLP-music_generation_input_output</id>
      <content type="html" xml:base="https://crosstar1228.github.io/NLP-music_generation_input_output">&lt;blockquote&gt;
  &lt;p&gt;Music representation은 크게 Symbol domain 과 audio domain으로 나뉩니다. 그 중 midi 파일의 표현 방식이자, discrete한 representation을 가지는
&lt;strong&gt;Symbolic music generation&lt;/strong&gt;에 대해 알아봅시다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;symbolic-music-domain&quot;&gt;Symbolic Music Domain&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;discrete 하고 표현이 명확함&lt;/li&gt;
  &lt;li&gt;dynamic 과 같은 악보 외적인 사항을 학습할 수 있음&lt;/li&gt;
  &lt;li&gt;특정악기에 customizing되어 있어서, 새로운 악기에 기존 modeling technique를 적용하는 것이 어려움&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1d-representation&quot;&gt;1D representation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;일반적인 표현 방법&lt;/li&gt;
  &lt;li&gt;note 간 관계 표현이 어려움(이전 note가 다음 note 시작까지 지속된다 등의 정보)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1-event-based&quot;&gt;1. Event-Based&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Midi-like event
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/063a5409-734b-4782-bde0-e7d71b66c7bb/image.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-sequence-based&quot;&gt;2. Sequence-Based&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;pitch, duration, chord 와 다른 음악 정보를 여러 list형태로 표현&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2d-representation&quot;&gt;2D representation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;sampling time에 의해 note를 2D matrix로 표현&lt;/li&gt;
  &lt;li&gt;더 높은 resolution을 필요로 함
    &lt;ul&gt;
      &lt;li&gt;rhythm이 복잡해지는 등 time dimension이 높아지면 장기적 의존관계를 학습하기 어려움
        &lt;h3 id=&quot;piano-roll&quot;&gt;Piano Roll&lt;/h3&gt;
        &lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/3856593a-eecb-410a-9ead-ff81f5893df4/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Automatic Piano에 의해 영감을 받음&lt;/li&gt;
  &lt;li&gt;(pitch 수 + resting state + holding state) * (resolution)모양
    &lt;ul&gt;
      &lt;li&gt;높이는 일반적으로 130개&lt;/li&gt;
      &lt;li&gt;너비는 일반적으로&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;하나의 matrix는 하나의 track을 의미하며, 여러 개의 track은 여러 matrix로 표현됨&lt;/li&gt;
  &lt;li&gt;pretty_midi 또는 Pypianoroll python toolkit이용하여 손쉽게 변환 가능&lt;/li&gt;
  &lt;li&gt;일반적으로 binary
    &lt;ul&gt;
      &lt;li&gt;각 position은 note가 그 position에서 연주되고 있는지 아닌지를 표현&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;note-off 정보가 없어서 16분음표 2개 반복과, 8분음표 하나를 같게 인식함&lt;/li&gt;
  &lt;li&gt;이를 개선하는 방법에는
    &lt;ul&gt;
      &lt;li&gt;hold replay&lt;/li&gt;
      &lt;li&gt;time step을 note beginning과 note end로 나눔&lt;/li&gt;
      &lt;li&gt;hold 를 표현하는 symbol인 ‘_’ 를 사용(monophonic melody에만 가능)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Conlon pianoroll : CNN에서 receptive field가 크지 않아도 되는 장점&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;A Comprehensive Survey on Deep Music Generation(2020)&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="nlp" />
      

      
        <summary type="html">Music representation은 크게 Symbol domain 과 audio domain으로 나뉩니다. 그 중 midi 파일의 표현 방식이자, discrete한 representation을 가지는 Symbolic music generation에 대해 알아봅시다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">A Comprehensive Survey on Deep Music Generation - Multi-level Representations, Algorithms, Evaluations, and Future Directions</title>
      <link href="https://crosstar1228.github.io/NLP-music_generation_paper_review_1" rel="alternate" type="text/html" title="A Comprehensive Survey on Deep Music Generation - Multi-level Representations, Algorithms, Evaluations, and Future Directions" />
      <published>2022-08-27T11:00:00+00:00</published>
      <updated>2022-08-27T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/NLP-music_generation_paper_review_1</id>
      <content type="html" xml:base="https://crosstar1228.github.io/NLP-music_generation_paper_review_1">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;딥러닝을 활용한 최근의 다양한 음악 작곡 TASK를 소개한 논문
    &lt;ul&gt;
      &lt;li&gt;DATASETS&lt;/li&gt;
      &lt;li&gt;MUSIC REPRESENTATION&lt;/li&gt;
      &lt;li&gt;EVALUATION METHOD&lt;/li&gt;
      &lt;li&gt;challenges &amp;amp; future directions&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;intro&quot;&gt;Intro&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;음악은 언어와 같다.&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;악보 : 음악  == 글 : 말&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Music Generation의 대분류 3개
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/33ed8e71-a0d8-4853-9c90-c9f1940900dc/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;score generation&lt;/li&gt;
      &lt;li&gt;performance generation add performance characteristics to scores
        &lt;ul&gt;
          &lt;li&gt;RENDERING performance&lt;/li&gt;
          &lt;li&gt;composing performance&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;audio generation : convert score with performance characteristics into audio
        &lt;ul&gt;
          &lt;li&gt;assign timbre&lt;/li&gt;
          &lt;li&gt;directry generate music in audio&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;그 중에서도 SCORE GENERATION이 가장 핫함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;style transfer 적용
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/3a730a16-e14a-4174-8d27-05807196346f/image.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;history&quot;&gt;History&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Iliac Suite : computer 로 만들어진 최초의 score
    &lt;ul&gt;
      &lt;li&gt;Markov chain 활용(Stochastic Model)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Coconet&lt;/strong&gt;
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/5b0df87e-0c0b-48b7-892a-b8958dd41752/image.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/e5825a0d-c3fd-450f-91f1-d034fe4c5c91/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;4마디 단위로 잘라서 멜로디에 alto,tenor,bass에 대응하는 멜로디 추가&lt;/li&gt;
      &lt;li&gt;piano roll type의 input을 one-hot vector로 encoding&lt;/li&gt;
      &lt;li&gt;16분음표로 quantization&lt;/li&gt;
      &lt;li&gt;전형적인 딥러닝의 multiclassification task&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Todd’s &lt;strong&gt;Time-Windowed&lt;/strong&gt; and conditioned recurrent architectures
    &lt;ol&gt;
      &lt;li&gt;인공신경망을 어떻게 음악생성에 활용할 수 있을지를 첫번쨰로 보여준 사례
        &lt;ol&gt;
          &lt;li&gt;input : context와 plan으로 나뉨
            &lt;ul&gt;
              &lt;li&gt;context :memory
      - bos + 각 note에 부합하는 unit으로 구성&lt;/li&gt;
              &lt;li&gt;plan : network가 학습한 특정 melody&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;학습방법
 &lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/78d3fb95-60f1-4f67-9b53-088ade4f5879/image.png&quot; alt=&quot;&quot; /&gt;
 1) plan에서 melody 와
 2) 초기화된 context의 첫번쨰 시점으로부터
 3) 첫번쨰 output을 냄 (비교 및 weight update)
 4) output은 다음 time step의 memory로 들어감
 -&amp;gt; iterative하게 output을 기억하여 생성!&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Lewis Creation by refinement&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;traditional-music-generation&quot;&gt;Traditional Music Generation&lt;/h3&gt;
&lt;p&gt;1) rule-based
    - linguistic &amp;amp; grammar 활용
    - 음악마다 다른 rule 이 만들어져야 하는 단점
    - 
2) Probability model&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;semi-automatic 한 알고리즘에 Markov chain이나 이외의 기술을 태움&lt;/li&gt;
  &lt;li&gt;원래의 data로부터 subsequence를 생성하지만, memory가 없어서 input에 따라 조건부 확률이 변동이 심함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3) Neural Network(RNN)
    - RNN으로 feature를 학습하여 단선율의 pitch와 duration을 학습
    - Gradient Vanishing으로 long-term dependency 학습 못하는 단점&lt;/p&gt;

&lt;h3 id=&quot;evloutionary&quot;&gt;evloutionary&lt;/h3&gt;
&lt;p&gt;1) GenJam
2) Director Musices
3) Probablistic&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;hierarchical HMM&lt;/li&gt;
  &lt;li&gt;dynamic Bayesian networks&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sound-modelling&quot;&gt;sound modelling&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;physical modeling&lt;/li&gt;
  &lt;li&gt;acoustic modeling : signal generator 이용
    &lt;ul&gt;
      &lt;li&gt;oscillator, modulator, filters와 같이 waveform 조작을 위한 processors&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lyrics&quot;&gt;lyrics&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;가사 포함 여부에 따라 music audio를 audio와 singing voice로 나눔.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-music-generation&quot;&gt;Deep Music Generation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Google Magenta : &lt;a href=&quot;https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn&quot;&gt;Melody RNN model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Anticipation-RNN : 사용자가 정의한 positional한 제약을 강요함.&lt;/li&gt;
  &lt;li&gt;TP-LSTM-NADE and BALSTM : 병렬 RNN으로부터 다선율의 데이터셋에서 translation-invariance를 유지&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;score-generation-vae-gan-transformer-&quot;&gt;Score Generation: VAE, GAN, Transformer, …&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;MusicVAE : longterm dependency, interpolation &amp;amp; reconstruction 퍼포먼스 굳
    &lt;ul&gt;
      &lt;li&gt;coupled latent variable model&lt;/li&gt;
      &lt;li&gt;binary regularizer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GAN
    &lt;ul&gt;
      &lt;li&gt;sequential data에는 학습에 어려움이 있음&lt;/li&gt;
      &lt;li&gt;CNN based GAN으로 개발&lt;/li&gt;
      &lt;li&gt;GAN-based MidiNet : Chord 에 condition되어 마디를 생성하는 알고리즘&lt;/li&gt;
      &lt;li&gt;MuseGAN : multi-track 다선율 음악을 생성하는 최초의 모델
        &lt;ul&gt;
          &lt;li&gt;강화학습을 접목하여 RNN-based GAN 적용&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transformer
    &lt;ul&gt;
      &lt;li&gt;REMI&lt;/li&gt;
      &lt;li&gt;Transformer XL&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;performance-generation--대부분-rnn&quot;&gt;Performance Generation : 대부분 RNN&lt;/h2&gt;

&lt;h2 id=&quot;representation&quot;&gt;Representation&lt;/h2&gt;
&lt;p&gt;1) symbol domain&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;discrete variable
2) audio domain&lt;/li&gt;
  &lt;li&gt;continuous variable&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;From artificial neural networks to deep learning for music generation: history, concepts and trends&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/generating-music-with-artificial-intelligence-9ce3c9eef806&lt;/li&gt;
  &lt;li&gt;https://topten.ai/music-generators-review/&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="nlp" />
      

      
        <summary type="html">Abstract 딥러닝을 활용한 최근의 다양한 음악 작곡 TASK를 소개한 논문 DATASETS MUSIC REPRESENTATION EVALUATION METHOD challenges &amp;amp; future directions</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">\[Paper Review\] Meshed-memory transformer for image captioning</title>
      <link href="https://crosstar1228.github.io/NLP-meshed_memory_transformer" rel="alternate" type="text/html" title="\[Paper Review\] Meshed-memory transformer for image captioning" />
      <published>2022-08-27T11:00:00+00:00</published>
      <updated>2022-08-27T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/NLP-meshed_memory_transformer</id>
      <content type="html" xml:base="https://crosstar1228.github.io/NLP-meshed_memory_transformer">&lt;h2 id=&quot;논문-간단-소개&quot;&gt;논문 간단 소개&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;image captioning task에서 transformer 모델을 활용한 모델 중 가장
    &lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
  &lt;/li&gt;
  &lt;li&gt;image encoding
    &lt;ul&gt;
      &lt;li&gt;학습된 사전 지식(caption)을 기반으로 image region간의 multi-level representation을 학습&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;language generation
    &lt;ul&gt;
      &lt;li&gt;low-level과 high-level feature를 모두 활용하는 mesh-like connectivity 활용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$M^2$ Transformer
    &lt;ul&gt;
      &lt;li&gt;다른 fully-attentive model과 비교해서 성능을 비교하고,&lt;/li&gt;
      &lt;li&gt;COCO image-text set에서 sota 기록&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;RNN + attention, Transformer, Bert 등 다양하게 이용되어 왔음&lt;/li&gt;
  &lt;li&gt;multimodal task는 기존 unimodal task와 다른 구조를 띠어야 함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/b1a321d2-0d39-49d6-881d-9eb273f3e5b6/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;encoding&quot;&gt;encoding&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MemoryAugmentedEncoding&lt;/code&gt;
    &lt;ol&gt;
      &lt;li&gt;이미지 구역 및 그 사이 관계가 multi-level(low-level, high-level) 로 인코딩됨&lt;/li&gt;
      &lt;li&gt;memory vector를 사용하여 사전지식을 encoding 및  관계를 모델링함&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;decoding&quot;&gt;decoding&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MeshedDecoding&lt;/code&gt;
    &lt;ol&gt;
      &lt;li&gt;학습된 gated mechanism으로 달성되는데, 각 단계별로 multi-level 의 기여도를 weight화함&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;→ encoder 와 decoder 간에 meshed connectivity 로 구조화됨&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;structure&quot;&gt;Structure&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/4f30ef85-a228-47d9-bac3-9a23b8781ddc/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;encoding-1&quot;&gt;Encoding&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;input image로부터 추출된 image region X (집합)&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  - 현재 image detection dataset을 활용하지만, 마디별 note 및 chord 범위로 설정 예정
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/49a747fe-4d6f-4383-ac83-2e9d820d7cd5/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;learnable weights W(q,k,v 에 대응되는)&lt;/li&gt;
  &lt;li&gt;S(X) = X의 weighted sum of values&lt;/li&gt;
  &lt;li&gt;image feature(region) 간의 pairwise similarity
    &lt;ul&gt;
      &lt;li&gt;하지만 이러한 방식의 self-attention은 사전 지식( a priori knowlendge)를 반영하지 못함&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;ex) 사람과 농구공이 있으면 player나 game과 같은 정보를 추론해내기 어려움&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1-memory-augmented-attention&quot;&gt;1. Memory-Augmented Attention&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;memory로 data augmentation이 진행되었다.&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;key 또는 value가 slot의 형태로 확장되고, 이는 사전지식을 encoding할 수 있음
      &lt;ul&gt;
        &lt;li&gt;이러한 slot은 learnable vector로서 SGD로 업데이트 가능(parameter)&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/fa37f3f2-0f75-46c7-9d13-ef4b2b917352/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$M_k, M_v$ 는 $n_m$ 개의 row수를 가진 학습가능한 matrix&lt;/li&gt;
  &lt;li&gt;, 는 concatenation을 의미&lt;/li&gt;
  &lt;li&gt;위 learnable parameter에 의해 X에 embedded 되지 않은 정보를 학습 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-encoding-layer&quot;&gt;2. Encoding layer&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/61222ccd-f0c5-41d8-84e3-11eb53d56b78/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이후 Memory augmented attention이 Position-Wise FFL 에 적용
    &lt;ul&gt;
      &lt;li&gt;2개의 affine transformation으로 이루어짐(non-linearity는 한곳에만 적용)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3-residual-connection--layer-norm&quot;&gt;3. Residual Connection + layer norm&lt;/h3&gt;

&lt;p&gt;각각의 sub-component(Memory-augmented attention 과 Encoding Layer)가 위 방식으로 감싸짐&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/34fe79cd-e000-41d1-acb7-40edc9bc5d97/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;AddNorm은 Residual Connection + Layer Normalization&lt;/p&gt;

&lt;h3 id=&quot;4-full-encoder&quot;&gt;4. Full encoder&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;여러 layer, 이전 레이어 아웃풋이 다음 레이어 인풋으로 들어감&lt;/li&gt;
  &lt;li&gt;더 높은 encoding layer는 이전에 이미 인식된 관계 정보를 사용할 수 있음&lt;/li&gt;
  &lt;li&gt;다양한 수준의 output을 생성&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;decoding-1&quot;&gt;Decoding&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/4f30ef85-a228-47d9-bac3-9a23b8781ddc/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;32-meshed-decoder&quot;&gt;3.2 Meshed Decoder&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;이전 step의 생성된 word와 region encoding&lt;/li&gt;
  &lt;li&gt;multi-layer structure&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;meshed-cross-attention&quot;&gt;meshed Cross attention&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;transformer의 cross-attention과 달리, 모든 encoding layer를 활용하여 생성 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/44e2c41e-8cde-4d63-9bd3-874033402a97/image.png&quot; alt=&quot;&quot; /&gt;
$C$ : encoder-decoder cross attention&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;decoder 로부터의 query와, encoder로부터의 key-value 쌍의 cross attention&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/cc5cd9d8-34e0-4e3b-bf96-1e2c382cbcaf/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\alpha_i$ : cross-attention 결과와 같은 크기의 weight matrix&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;각 encoding layer의 기여도 및 상대적인 중요도를 조절해주는 값&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/5cbb125d-5ded-454a-b644-3878ff311e37/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;input query와 Cross Attention output간의 관련성 측정한 결과값&lt;/li&gt;
  &lt;li&gt;$W_i$ 는 2d*d matrix&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;masked-self-attention&quot;&gt;Masked Self attention&lt;/h3&gt;

&lt;p&gt;$S_mask$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;input sequence Y의 t번째 element로부터의 query&lt;/li&gt;
  &lt;li&gt;왼쪽의 subsequence로부터 얻은 key와 value&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ffw--addnorm&quot;&gt;FFW + AddNorm&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/b237802b-2b01-49fa-802d-2bc333b54d38/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;여러 layer로 stack&lt;/li&gt;
  &lt;li&gt;결과적으로 t 번째 element에 기반하여 t+1 번째 시점이 예측됨&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;XE(word-level Crossentropy loss)로 pre-training
    &lt;ul&gt;
      &lt;li&gt;
        &lt;h2 id=&quot;ground-truth-word를-기반으로-다음-token을-예측&quot;&gt;Ground-truth word를 기반으로 다음 token을 예측&lt;/h2&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reinforcement learning 활용한 예측
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;self-critical sequence training approach&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;topk&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataset--coco&quot;&gt;Dataset : COCO&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;120,000 image with 5 captions each&lt;/li&gt;
  &lt;li&gt;nocap : 15,100 images annotated with 11 human-generated captions&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;From artificial neural networks to deep learning for music generation: history, concepts and trends&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/generating-music-with-artificial-intelligence-9ce3c9eef806&lt;/li&gt;
  &lt;li&gt;https://topten.ai/music-generators-review/&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="nlp" />
      

      
        <summary type="html">논문 간단 소개 image captioning task에서 transformer 모델을 활용한 모델 중 가장 Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">(Paper Review) AudioLM ; a language Modeling Approach to Audio Generation</title>
      <link href="https://crosstar1228.github.io/NLP-AudioLM" rel="alternate" type="text/html" title="(Paper Review) AudioLM ; a language Modeling Approach to Audio Generation" />
      <published>2022-08-27T11:00:00+00:00</published>
      <updated>2022-08-27T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/NLP-AudioLM</id>
      <content type="html" xml:base="https://crosstar1228.github.io/NLP-AudioLM">&lt;h2 id=&quot;논문-간단-소개&quot;&gt;논문 간단 소개&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;Google Research 에서 개발한 모델. Semantic 과 acuostic, 두 갈래의 tokenization으로 나누어 성능 향샹을 이룸&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Transcript(대본)을 통한 audio generation process가 주를 이루어왔음
    &lt;ul&gt;
      &lt;li&gt;speech synthesis에서 텍스트 대본&lt;/li&gt;
      &lt;li&gt;피아노에서 midi Representation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;하지만 transcript가 없는 상황에서는 생성에 어려움을 겪음
    &lt;ul&gt;
      &lt;li&gt;speech voice recover : speaker characteristic 이 필요함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;auodiolm의-허들&quot;&gt;AuodioLM의 허들&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;대규모 말뭉치를 학습한 language model들은 최근 좋은 성능을 보임&lt;/li&gt;
  &lt;li&gt;AudioLM은 그러한 진보에 박차를 가하여(그 구조를 따와서) &lt;strong&gt;annotated data 없이 audio를 생성할 수 있음&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;하지만 language model로부터 audio language model로 가려면 허들이 존재함
    &lt;ol&gt;
      &lt;li&gt;audio 생성을 위해서는 language 생성보다 더 밀도있는 데이터가 필요함&lt;/li&gt;
      &lt;li&gt;text와 audio는 one-to-many 관계에 있어서, 같은 문장도 speaker, 감정, 스타일에 따라 달라짐&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;이러한-허들을-극복하기-위하여&quot;&gt;이러한 허들을 극복하기 위하여&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/71e3f8d5-0325-4df0-a867-096b632377c1/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;semantic token (from &lt;a href=&quot;https://deepai.org/publication/w2v-bert-combining-contrastive-learning-and-masked-language-modeling-for-self-supervised-speech-pre-training&quot;&gt;W2v-BERT&lt;/a&gt;)
    &lt;ul&gt;
      &lt;li&gt;local dependency(speech의 음소, 음악의 melody)&lt;/li&gt;
      &lt;li&gt;global long-term structure(speech의 맥락, 음악의 harmony나 rhythm)&lt;/li&gt;
      &lt;li&gt;긴 sequence를 허락하기 위하여 audio signal을 downsampling함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;acoustic token (from &lt;a href=&quot;https://ai.googleblog.com/2021/08/soundstream-end-to-end-neural-audio.html&quot;&gt;SoundStream neural codec&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;audio-only-language-model&quot;&gt;Audio-Only language model&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;AudioLM은 audio data만을 학습(text나 symbolic representation을 학습하지 않음)&lt;/li&gt;
  &lt;li&gt;Hierachical한 모델(high level-&amp;gt; low level)
    &lt;ul&gt;
      &lt;li&gt;semantic token -&amp;gt; coarse acoustic token -&amp;gt; fine acoustic token으로 chainin하는 구조&lt;/li&gt;
      &lt;li&gt;chain 사이사이에 transformer 모델이 활용됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1-semantic-token&quot;&gt;1. Semantic token&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/8814e6d5-706b-4fd9-8683-85bba144938e/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;language model과 같이 next token을 예측하는 구조&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-coarse-acoustic-token&quot;&gt;2. coarse acoustic token&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/a915f356-ab6e-46ba-8c9d-b3114e45318f/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Genrerated token 과 poast coarse acoustic token을 concat함&lt;/li&gt;
  &lt;li&gt;이러한 형태로 acoustic token이 예측됨&lt;/li&gt;
  &lt;li&gt;이 단계에서 speaker의 특성(음색 등)이 모델링됨&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3-fine-acoustic-token&quot;&gt;3. fine acoustic token&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/329274e9-1577-4224-b561-e2b18b4b269b/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;최종 audio에 detail을 추가함&lt;/li&gt;
  &lt;li&gt;최종 acoustic token을 Soundstream decoder를 통해 reconstruct&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;결론적으로&quot;&gt;결론적으로&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;piano continumation ,speech continuation, unconditional generation 등의 task에서 좋은 성능을 냄&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://google-research.github.io/seanet/audiolm/examples/&quot;&gt;이 페이지&lt;/a&gt;에서 디테일한 확인이 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="nlp" />
      

      
        <summary type="html">논문 간단 소개 Google Research 에서 개발한 모델. Semantic 과 acuostic, 두 갈래의 tokenization으로 나누어 성능 향샹을 이룸</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Paper Review - Identifying and attacking the saddle point problem in high-dimensional non-convex optimization</title>
      <link href="https://crosstar1228.github.io/local_minima_saddle_point" rel="alternate" type="text/html" title="Paper Review - Identifying and attacking the saddle point problem in high-dimensional non-convex optimization" />
      <published>2022-08-11T11:00:00+00:00</published>
      <updated>2022-08-11T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/local_minima_saddle_point</id>
      <content type="html" xml:base="https://crosstar1228.github.io/local_minima_saddle_point">&lt;blockquote&gt;
  &lt;p&gt;Keyword : Saddle Point, SFN(Saddle-Free Newton Method) SGD&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/3666f71f-bc42-4c8a-baf5-4ca4cb767c30/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;1) 고차원의 non-convex function에서는 포괄적으로(generically) saddle point가 문제되는 경우가 많다
2) 고차원의 non-convex function에서는 대체로 local minima 문제는 통계적/경험적으로 매우 드문 경우이다
3) 이에 따라 일반화된 trust region 방법이 개발됨
   1) 도함수가 아닌 신뢰 지역(trust region)의 모양을 감지
   2) 곡률 정보를 알 수 있음&lt;/p&gt;
&lt;h3 id=&quot;saddle-point-에서-벗어나기&quot;&gt;Saddle point 에서 벗어나기&lt;/h3&gt;
&lt;p&gt;4) Saddle-Free Method : Hessian 값의 역으로 절댓값의 gradient value를 재조정
    - 이 방법으로 Gradient Descent와 Newton Method를 적절이 섞어 사용함으로서 빠르게 saddle point에서 벗어날 수 있음&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://papers.nips.cc/paper/2014/file/17e23e50bedc63b4095e3d8204ce063b-Paper.pdf&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="ML" />
      

      
        <summary type="html">Keyword : Saddle Point, SFN(Saddle-Free Newton Method) SGD</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Generative Model의 신흥강자, Diffusion Model!</title>
      <link href="https://crosstar1228.github.io/ML-Diffusion_Model" rel="alternate" type="text/html" title="Generative Model의 신흥강자, Diffusion Model!" />
      <published>2022-07-29T11:00:00+00:00</published>
      <updated>2022-07-29T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/ML-Diffusion_Model</id>
      <content type="html" xml:base="https://crosstar1228.github.io/ML-Diffusion_Model">&lt;blockquote&gt;
  &lt;p&gt;이번 시간에 제가 소개해볼 논문이자 인공지능 모델은, 2021년 NeurIPS(신겅정보처리시스템학회) 에서 발표한 모델이자 생성모델 분야의 뜨거운 감자, &lt;strong&gt;Diffusion Model&lt;/strong&gt;입니다!
 이름이 왜 Diffusion Model인지, 모델 구조와 최적화 Metric을 유념하면서 AI 생성모델 분야에 어떠한 영향력을 미치고 있는지 한번 알아보도록 해요!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;generative-model&quot;&gt;Generative Model?&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;주어진 관측 데이터 x 로부터 추출된 분포(distribution)을 평가하는 모델! Autoregressive Model부터 VAE, GAN, Flow - Based Model 등 다양!&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;diffusion-model이어야-하는-이유&quot;&gt;Diffusion model이어야 하는 이유?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;2021 NeurIPS 학회에서 Autoregressive 계열 생성모델보다 likelihood 성능이 좋고, GAN Based model보다 quality가 높은 sample 을 생성하는 것으로 발표되었답니다!&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://paperswithcode.com/paper/diffusion-models-beat-gans-on-image-synthesis&quot;&gt;Diffusion Models Beat GANs on Image Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;diffusion-model&quot;&gt;Diffusion Model&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;점진적인 noise 추가의 역과정을 학습하는 것이 개념의 핵심&lt;/li&gt;
  &lt;li&gt;주어진 데이터 x 로부터 noise를 추가하고, noise data로부터 x로 돌아오는 과정을 학습함&lt;/li&gt;
  &lt;li&gt;VAE와 컨셉은 유사하지만, data 분포를 학습하는 것이 아니라 Markov Chain 안에서 noise 분포를 모델링하는 Latent variable model
    &lt;ul&gt;
      &lt;li&gt;여기서 markov chain이란, 특정한 확률적인 규칙에 의해 하나의 상태에서 다른 특정한 상태로 변화하는 수학적 계(界)를 의미함. 쉽게 말해, 확률에 의한 시간에 따른 상태 변화.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;prior로부터 posterior를 얻기 위하여 x1,x2, xT 의 각 time point 별 latent varible에 noise를 더해가는 구조
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/30bdb629-19dc-4948-a4fb-59d25fd09ddd/image.png&quot; alt=&quot;&quot; /&gt;
noise를 추가하는 과정을 diffusion(확산 또는 전파)라고 이해할 수 있겠죠. 
 $x_T$ 로부터 시간에 따라 조건부 확률에 의해 noise로부터 원래 이미지를 복원해나가는 모습입니다.&lt;/li&gt;
  &lt;li&gt;계충적(hierachical)한 방법으로 data를 denoise, 다시 말해 decode를 하는 것이죠.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;process&quot;&gt;process&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/2917dd77-3028-43f5-9348-5f2ea581f471/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;forward process&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;점진적으로 noise를 더해가는 과정 $q$&lt;/li&gt;
      &lt;li&gt;real data의 distribution을 $q(x_0)$라고 한다면, sampling을 할 수 있음. ($x_0$ ~ $q(x_0)$)
        &lt;ul&gt;
          &lt;li&gt;time step 별로 gaussian noise를 추가하는 과정은 아래와 같다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$q(x_t|x_{t-1}) = N(x_t ; \sqrt{1-\beta_t}x_{t-1}, \beta_tI)$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;여기서 $\beta$ 는 variance schedule(&lt;strong&gt;학습되기도 하고(parameter), 고정되기도 함(hyperparameter)&lt;/strong&gt;)이라고 해서, 직관적으로는 time step별로 추가될  noise의 분산에 해당하는 값이다.&lt;/li&gt;
      &lt;li&gt;diffusion model을 하나의 함수라고 한다면, model은 noisy component를 $\epsilon(x_t, t)$
        &lt;ul&gt;
          &lt;li&gt;true noise 와 predicted noise의 차이&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;reverse process&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;학습된&lt;/strong&gt; denoising process $p_\theta$&lt;/li&gt;
      &lt;li&gt;원래 $p(x_{t-1}|x_t)$ 의 conditional distribution에 가까운 $p_\theta(x_{t-1}|x_t)$ 를 학습하는 것!
        &lt;ul&gt;
          &lt;li&gt;신경망이 위에 해당하는 parameter를 학습하고 loss 파악하고 gradient descent 로 update하는 과정!&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;역과정도 Gaussian noise를 가정하므로, 정규분포의 평균과 분산이 parameter로 정의됨
        &lt;ul&gt;
          &lt;li&gt;$p_\theta(x_{t-1}|x_t) = N(x_{t_1}; \mu_\theta(x_t, t), \sigma_\theta(x_t, t))$&lt;/li&gt;
          &lt;li&gt;허나 DDPM저자는 variance를 고정시키고, 조건부 확률 분포의 &lt;strong&gt;평균&lt;/strong&gt;만 학습하게 함. 비슷한 결과를 보이기 때문. (이후 더 발전된 형태에서는 variance만 학습)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;denoising 과정이 학습이 되어있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;반복-과정&quot;&gt;반복 과정!&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/6681b933-1134-44f6-abcd-3602ddde8ba1/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;benefits&quot;&gt;Benefits&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;scalability&lt;/li&gt;
  &lt;li&gt;parallelizabliity&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;objective-function&quot;&gt;objective function&lt;/h3&gt;
&lt;p&gt;negative log likelihood를 사용하며, 초기 이미지(ground truth)의 확률분포와 복원된 이미지의 확률분포 간 KL Divergence&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;둘다 Gaussian distribution&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;각 time step의 Loss 를 모두 합한 것이 최종 loss가 됨&lt;/p&gt;

&lt;h3 id=&quot;variance-schedule이-학습-가능한-경우&quot;&gt;variance schedule이 학습 가능한 경우&lt;/h3&gt;
&lt;p&gt;각 time step의 loss 는 아래와 같이 정의되고,
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/dd20c074-9cd6-44d6-8134-9e32e6743f0f/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;kl-divergence&quot;&gt;KL divergence?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;쉽게 이야기해서, 확률분포 간의 어긋난 정도
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/562b198d-c1ea-4cd6-968d-a5e4ff685a95/image.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;variance-schedule을-고정시키면&quot;&gt;variance schedule을 고정시키면!&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;schedule을 고정시켜도 성능이 충분히 좋다&lt;/li&gt;
  &lt;li&gt;선형 나열보다는 기하학적 나열이 더 성능이 좋음&lt;/li&gt;
  &lt;li&gt;$L_{t-1}$ = $|| \epsilon - \epsilon_\theta(x_t, t) ||^2$&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;h3 id=&quot;정리-및-마무리&quot;&gt;정리 및 마무리&lt;/h3&gt;
  &lt;ul&gt;
    &lt;li&gt;forward process에서 markov chain에 의한 gaussian noise를 추가하고, 특정 variance schedule을 따름&lt;/li&gt;
    &lt;li&gt;reverse process에서 parameter에 의해 원본 복원과정 학습&lt;/li&gt;
    &lt;li&gt;모델 평가는, step별 loss의 합.&lt;/li&gt;
    &lt;li&gt;특정 time step의 복원된 확률분포 $p$ 와 생성된 확률분포 $q$ 간의 차이를 비교함. KL Divregence 또는 gaussian distribution에 대응되는 noise간의 rmse score&lt;/li&gt;
    &lt;li&gt;유연성이 좋고 다양한 모델에 적용 가능. 성능도 좋음 (ex, DALLE-2)&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/pdf/2006.11239.pdf&lt;/li&gt;
  &lt;li&gt;https://www.lgresearch.ai/kor/blog/view/?seq=190&amp;amp;page=1&amp;amp;pageSize=12&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="ML" />
      

      
        <summary type="html">이번 시간에 제가 소개해볼 논문이자 인공지능 모델은, 2021년 NeurIPS(신겅정보처리시스템학회) 에서 발표한 모델이자 생성모델 분야의 뜨거운 감자, Diffusion Model입니다! 이름이 왜 Diffusion Model인지, 모델 구조와 최적화 Metric을 유념하면서 AI 생성모델 분야에 어떠한 영향력을 미치고 있는지 한번 알아보도록 해요!</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Music Generation and AI, present and future</title>
      <link href="https://crosstar1228.github.io/NLP-music_and_ai" rel="alternate" type="text/html" title="Music Generation and AI, present and future" />
      <published>2022-07-18T11:00:00+00:00</published>
      <updated>2022-07-18T11:00:00+00:00</updated>
      <id>https://crosstar1228.github.io/NLP-music_and_ai</id>
      <content type="html" xml:base="https://crosstar1228.github.io/NLP-music_and_ai">&lt;h2 id=&quot;목차&quot;&gt;목차&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://velog.io/@tobigsvoice1516/5%EC%A3%BC%EC%B0%A8-MUSIC-COMPOSITION-WITH-DEEP-LEARNING-A-REVIEW&quot;&gt;music &amp;amp; ai 역사&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openai.com/blog/jukebox/&quot;&gt;OpenAI - JukeBox&lt;/a&gt; [&lt;a href=&quot;https://github.com/openai/jukebox/&quot;&gt;Github&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;Datasets&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ai-music-genration의-시초&quot;&gt;AI Music Genration의 시초&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;90년대 David Bowie 의 Verbasizer (앱)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;단어를 임의로 재배치하여 음악 가사에 사용될 수 있도록 재조합하는 앱이었음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;2016년 Sony의 App Flow Machine
    &lt;ul&gt;
      &lt;li&gt;비틀즈 스타일 멜로디를 창조해 냄&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;music-generation-도-크게-다르지-않아&quot;&gt;Music Generation 도 크게 다르지 않아&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;머신러닝에서 모델은 다량의 데이터를 학습하고 그 안에서 ‘패턴’을 찾아냅니다.&lt;/li&gt;
  &lt;li&gt;Music Generation에서는 그 패턴이 Chord, Tempo, lengths, note 간 관계성 등 이됩니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;symbolic-approach-non-symbolic-approach&quot;&gt;Symbolic approach, Non-symbolic approach&lt;/h3&gt;

&lt;h2 id=&quot;music-generation의-고질적인-문제-1--long-term-dependency&quot;&gt;Music Generation의 고질적인 문제 1 : LONG TERM DEPENDENCY&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;해결법 1 : autoencoder로 저차원 space로 mapping
    &lt;ul&gt;
      &lt;li&gt;불필요한 정보를 버리게 됨&lt;/li&gt;
      &lt;li&gt;이후 upsampling&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MuseNet : midi data 기반 많은 양의 데이터 학습&lt;/li&gt;
  &lt;li&gt;Transfomer 계열 모델로 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;아이디어-&quot;&gt;아이디어 :&lt;/h3&gt;
&lt;p&gt;o learn a lower-dimensional encoding of the audio with the goal of losing the less important
information but retaining most of the musical information&lt;/p&gt;
&lt;h2 id=&quot;문제-2--diversityvariation&quot;&gt;문제 2 : Diversity(variation)&lt;/h2&gt;

&lt;h3 id=&quot;jukeboxpaper&quot;&gt;JukeBox[&lt;a href=&quot;https://arxiv.org/abs/2005.00341&quot;&gt;Paper&lt;/a&gt;]&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;long context 를 autoregressiveTransformer 이용한 multi-sclae VQ-VAE로 해결&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lyric-conditioning&quot;&gt;Lyric Conditioning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;노래의 duration에 linear 하게 가사의 문자들을 align하는 방법&lt;/li&gt;
  &lt;li&gt;가사를 위한 encoder를 더하고, &lt;strong&gt;music decoder로부터 의 query&lt;/strong&gt;로부터 &lt;strong&gt;가사 encoder로부터의 key, value 쌍&lt;/strong&gt; 으로의 attetion layer를 적용함.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vq-vae-codebook-collapse&quot;&gt;VQ-VAE codebook collapse&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;codebook에 mapping된 embedding vector들이 많이 쓰이지 않는 현상&lt;/li&gt;
  &lt;li&gt;Random Restart:codebook vector 사용량이 평균이하로 떨어지면 , encoder output 중 하나로 다시 reset&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;https://magenta.tensorflow.org/perceiver-ar&lt;/p&gt;

&lt;h3 id=&quot;sparse-transformer&quot;&gt;Sparse Transformer&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;sparsifies the attention pattern by
reshaping the input sequence into a &lt;strong&gt;2-D sequence&lt;/strong&gt; of
shape&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;google-deepmind-202206&quot;&gt;Google Deepmind (2022.06)&lt;/h2&gt;
&lt;p&gt;[&lt;a href=&quot;https://arxiv.org/abs/2202.07765&quot;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;h3 id=&quot;perceiver-ar&quot;&gt;Perceiver AR&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;modality 에 대하여 agnostic(인지불능)인 구조
    &lt;ul&gt;
      &lt;li&gt;cross attention : long-range input -&amp;gt; small latent&lt;/li&gt;
      &lt;li&gt;maintaining end-to-end causal masking
https://soundraw.io/ 
https://magenta.tensorflow.org/
https://www.aiva.ai/
-&amp;gt; 음악 작곡&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Datasets&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://paperswithcode.com/task/music-generation&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;music-generation-and-deep-learning&quot;&gt;Music Generation and Deep learning&lt;/h2&gt;
&lt;p&gt;1) 딥러닝 베이스 음악 생성의 컨셉
2) 음악 생성의 다양한 방법과 원리
3) 다양한 음악 생성의 개념적 분류 체계
4) 트렌드&lt;/p&gt;

&lt;p&gt;abstract model이 generation을 위해 사용됨&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sota models
MuseGAN
Melnet
MidiNet&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;From artificial neural networks to deep learning for music generation: history, concepts and trends&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/generating-music-with-artificial-intelligence-9ce3c9eef806&lt;/li&gt;
  &lt;li&gt;https://topten.ai/music-generators-review/&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>건너별(crosstar)</name>
        
        
      </author>

      

      
        <category term="nlp" />
      

      
        <summary type="html">목차 music &amp;amp; ai 역사 OpenAI - JukeBox [Github] Datasets</summary>
      

      
      
    </entry>
  
</feed>
