<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://crosstar1228.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://crosstar1228.github.io/" rel="alternate" type="text/html" /><updated>2022-09-26T14:06:37+00:00</updated><id>https://crosstar1228.github.io/feed.xml</id><title type="html">건너별의 Romantic AI</title><subtitle>IT/인공지능 서랍장</subtitle><entry><title type="html"></title><link href="https://crosstar1228.github.io/2022-09-24-NLP-meshed_memory_transformer" rel="alternate" type="text/html" title="" /><published>2022-09-26T14:06:37+00:00</published><updated>2022-09-26T14:06:37+00:00</updated><id>https://crosstar1228.github.io/2022-09-24-NLP-meshed_memory_transformer</id><content type="html" xml:base="https://crosstar1228.github.io/2022-09-24-NLP-meshed_memory_transformer">&lt;h2 id=&quot;논문-간단-소개&quot;&gt;논문 간단 소개&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;image captioning task에서 transformer 모델을 활용한 모델 중 가장
    &lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
  &lt;/li&gt;
  &lt;li&gt;image encoding
    &lt;ul&gt;
      &lt;li&gt;학습된 사전 지식(caption)을 기반으로 image region간의 multi-level representation을 학습&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;language generation
    &lt;ul&gt;
      &lt;li&gt;low-level과 high-level feature를 모두 활용하는 mesh-like connectivity 활용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$M^2$ Transformer
    &lt;ul&gt;
      &lt;li&gt;다른 fully-attentive model과 비교해서 성능을 비교하고,&lt;/li&gt;
      &lt;li&gt;COCO image-text set에서 sota 기록&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;RNN + attention, Transformer, Bert 등 다양하게 이용되어 왔음&lt;/li&gt;
  &lt;li&gt;multimodal task는 기존 unimodal task와 다른 구조를 띠어야 함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/b1a321d2-0d39-49d6-881d-9eb273f3e5b6/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;encoding&quot;&gt;encoding&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MemoryAugmentedEncoding&lt;/code&gt;
    &lt;ol&gt;
      &lt;li&gt;이미지 구역 및 그 사이 관계가 multi-level(low-level, high-level) 로 인코딩됨&lt;/li&gt;
      &lt;li&gt;memory vector를 사용하여 사전지식을 encoding 및  관계를 모델링함&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;decoding&quot;&gt;decoding&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MeshedDecoding&lt;/code&gt;
    &lt;ol&gt;
      &lt;li&gt;학습된 gated mechanism으로 달성되는데, 각 단계별로 multi-level 의 기여도를 weight화함&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;→ encoder 와 decoder 간에 meshed connectivity 로 구조화됨&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;structure&quot;&gt;Structure&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/4f30ef85-a228-47d9-bac3-9a23b8781ddc/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;encoding-1&quot;&gt;Encoding&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;input image로부터 추출된 image region X (집합)&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  - 현재 image detection dataset을 활용하지만, 마디별 note 및 chord 범위로 설정 예정
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/49a747fe-4d6f-4383-ac83-2e9d820d7cd5/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;learnable weights W(q,k,v 에 대응되는)&lt;/li&gt;
  &lt;li&gt;S(X) = X의 weighted sum of values&lt;/li&gt;
  &lt;li&gt;image feature(region) 간의 pairwise similarity
    &lt;ul&gt;
      &lt;li&gt;하지만 이러한 방식의 self-attention은 사전 지식( a priori knowlendge)를 반영하지 못함&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;ex) 사람과 농구공이 있으면 player나 game과 같은 정보를 추론해내기 어려움&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1-memory-augmented-attention&quot;&gt;1. Memory-Augmented Attention&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;memory로 data augmentation이 진행되었다.&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;key 또는 value가 slot의 형태로 확장되고, 이는 사전지식을 encoding할 수 있음
      &lt;ul&gt;
        &lt;li&gt;이러한 slot은 learnable vector로서 SGD로 업데이트 가능(parameter)&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/fa37f3f2-0f75-46c7-9d13-ef4b2b917352/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$M_k, M_v$ 는 $n_m$ 개의 row수를 가진 학습가능한 matrix&lt;/li&gt;
  &lt;li&gt;, 는 concatenation을 의미&lt;/li&gt;
  &lt;li&gt;위 learnable parameter에 의해 X에 embedded 되지 않은 정보를 학습 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-encoding-layer&quot;&gt;2. Encoding layer&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/61222ccd-f0c5-41d8-84e3-11eb53d56b78/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이후 Memory augmented attention이 Position-Wise FFL 에 적용
    &lt;ul&gt;
      &lt;li&gt;2개의 affine transformation으로 이루어짐(non-linearity는 한곳에만 적용)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3-residual-connection--layer-norm&quot;&gt;3. Residual Connection + layer norm&lt;/h3&gt;

&lt;p&gt;각각의 sub-component(Memory-augmented attention 과 Encoding Layer)가 위 방식으로 감싸짐&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/34fe79cd-e000-41d1-acb7-40edc9bc5d97/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;AddNorm은 Residual Connection + Layer Normalization&lt;/p&gt;

&lt;h3 id=&quot;4-full-encoder&quot;&gt;4. Full encoder&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;여러 layer, 이전 레이어 아웃풋이 다음 레이어 인풋으로 들어감&lt;/li&gt;
  &lt;li&gt;더 높은 encoding layer는 이전에 이미 인식된 관계 정보를 사용할 수 있음&lt;/li&gt;
  &lt;li&gt;다양한 수준의 output을 생성&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;decoding-1&quot;&gt;Decoding&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/4f30ef85-a228-47d9-bac3-9a23b8781ddc/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;32-meshed-decoder&quot;&gt;3.2 Meshed Decoder&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;이전 step의 생성된 word와 region encoding&lt;/li&gt;
  &lt;li&gt;multi-layer structure&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;meshed-cross-attention&quot;&gt;meshed Cross attention&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;transformer의 cross-attention과 달리, 모든 encoding layer를 활용하여 생성 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/44e2c41e-8cde-4d63-9bd3-874033402a97/image.png&quot; alt=&quot;&quot; /&gt;
$C$ : encoder-decoder cross attention&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;decoder 로부터의 query와, encoder로부터의 key-value 쌍의 cross attention&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/cc5cd9d8-34e0-4e3b-bf96-1e2c382cbcaf/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\alpha_i$ : cross-attention 결과와 같은 크기의 weight matrix&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;각 encoding layer의 기여도 및 상대적인 중요도를 조절해주는 값&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/5cbb125d-5ded-454a-b644-3878ff311e37/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;input query와 Cross Attention output간의 관련성 측정한 결과값&lt;/li&gt;
  &lt;li&gt;$W_i$ 는 2d*d matrix&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;masked-self-attention&quot;&gt;Masked Self attention&lt;/h3&gt;

&lt;p&gt;$S_mask$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;input sequence Y의 t번째 element로부터의 query&lt;/li&gt;
  &lt;li&gt;왼쪽의 subsequence로부터 얻은 key와 value&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ffw--addnorm&quot;&gt;FFW + AddNorm&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/b237802b-2b01-49fa-802d-2bc333b54d38/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;여러 layer로 stack&lt;/li&gt;
  &lt;li&gt;결과적으로 t 번째 element에 기반하여 t+1 번째 시점이 예측됨&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;XE(word-level Crossentropy loss)로 pre-training
    &lt;ul&gt;
      &lt;li&gt;
        &lt;h2 id=&quot;ground-truth-word를-기반으로-다음-token을-예측&quot;&gt;Ground-truth word를 기반으로 다음 token을 예측&lt;/h2&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reinforcement learning 활용한 예측
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;self-critical sequence training approach&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;topk&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataset--coco&quot;&gt;Dataset : COCO&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;120,000 image with 5 captions each&lt;/li&gt;
  &lt;li&gt;nocap : 15,100 images annotated with 11 human-generated captions&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;From artificial neural networks to deep learning for music generation: history, concepts and trends&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/generating-music-with-artificial-intelligence-9ce3c9eef806&lt;/li&gt;
  &lt;li&gt;https://topten.ai/music-generators-review/&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author></entry><entry><title type="html">Symbolic Music Representation - 1D, 2D Future Directions</title><link href="https://crosstar1228.github.io/NLP-music_generation_input_output" rel="alternate" type="text/html" title="Symbolic Music Representation - 1D, 2D Future Directions" /><published>2022-09-13T11:00:00+00:00</published><updated>2022-09-13T11:00:00+00:00</updated><id>https://crosstar1228.github.io/NLP-music_generation_input_output</id><content type="html" xml:base="https://crosstar1228.github.io/NLP-music_generation_input_output">&lt;blockquote&gt;
  &lt;p&gt;Music representation은 크게 Symbol domain 과 audio domain으로 나뉩니다. 그 중 midi 파일의 표현 방식이자, discrete한 representation을 가지는
&lt;strong&gt;Symbolic music generation&lt;/strong&gt;에 대해 알아봅시다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;symbolic-music-domain&quot;&gt;Symbolic Music Domain&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;discrete 하고 표현이 명확함&lt;/li&gt;
  &lt;li&gt;dynamic 과 같은 악보 외적인 사항을 학습할 수 있음&lt;/li&gt;
  &lt;li&gt;특정악기에 customizing되어 있어서, 새로운 악기에 기존 modeling technique를 적용하는 것이 어려움&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1d-representation&quot;&gt;1D representation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;일반적인 표현 방법&lt;/li&gt;
  &lt;li&gt;note 간 관계 표현이 어려움(이전 note가 다음 note 시작까지 지속된다 등의 정보)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1-event-based&quot;&gt;1. Event-Based&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Midi-like event
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/063a5409-734b-4782-bde0-e7d71b66c7bb/image.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-sequence-based&quot;&gt;2. Sequence-Based&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;pitch, duration, chord 와 다른 음악 정보를 여러 list형태로 표현&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2d-representation&quot;&gt;2D representation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;sampling time에 의해 note를 2D matrix로 표현&lt;/li&gt;
  &lt;li&gt;더 높은 resolution을 필요로 함
    &lt;ul&gt;
      &lt;li&gt;rhythm이 복잡해지는 등 time dimension이 높아지면 장기적 의존관계를 학습하기 어려움
        &lt;h3 id=&quot;piano-roll&quot;&gt;Piano Roll&lt;/h3&gt;
        &lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/3856593a-eecb-410a-9ead-ff81f5893df4/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Automatic Piano에 의해 영감을 받음&lt;/li&gt;
  &lt;li&gt;(pitch 수 + resting state + holding state) * (resolution)모양
    &lt;ul&gt;
      &lt;li&gt;높이는 일반적으로 130개&lt;/li&gt;
      &lt;li&gt;너비는 일반적으로&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;하나의 matrix는 하나의 track을 의미하며, 여러 개의 track은 여러 matrix로 표현됨&lt;/li&gt;
  &lt;li&gt;pretty_midi 또는 Pypianoroll python toolkit이용하여 손쉽게 변환 가능&lt;/li&gt;
  &lt;li&gt;일반적으로 binary
    &lt;ul&gt;
      &lt;li&gt;각 position은 note가 그 position에서 연주되고 있는지 아닌지를 표현&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;note-off 정보가 없어서 16분음표 2개 반복과, 8분음표 하나를 같게 인식함&lt;/li&gt;
  &lt;li&gt;이를 개선하는 방법에는
    &lt;ul&gt;
      &lt;li&gt;hold replay&lt;/li&gt;
      &lt;li&gt;time step을 note beginning과 note end로 나눔&lt;/li&gt;
      &lt;li&gt;hold 를 표현하는 symbol인 ‘_’ 를 사용(monophonic melody에만 가능)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Conlon pianoroll : CNN에서 receptive field가 크지 않아도 되는 장점&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;A Comprehensive Survey on Deep Music Generation(2020)&lt;/li&gt;
&lt;/ul&gt;</content><author><name>건너별(crosstar)</name></author><category term="nlp" /><summary type="html">Music representation은 크게 Symbol domain 과 audio domain으로 나뉩니다. 그 중 midi 파일의 표현 방식이자, discrete한 representation을 가지는 Symbolic music generation에 대해 알아봅시다.</summary></entry><entry><title type="html">A Comprehensive Survey on Deep Music Generation - Multi-level Representations, Algorithms, Evaluations, and Future Directions</title><link href="https://crosstar1228.github.io/NLP-music_generation_paper_review_1" rel="alternate" type="text/html" title="A Comprehensive Survey on Deep Music Generation - Multi-level Representations, Algorithms, Evaluations, and Future Directions" /><published>2022-08-27T11:00:00+00:00</published><updated>2022-08-27T11:00:00+00:00</updated><id>https://crosstar1228.github.io/NLP-music_generation_paper_review_1</id><content type="html" xml:base="https://crosstar1228.github.io/NLP-music_generation_paper_review_1">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;딥러닝을 활용한 최근의 다양한 음악 작곡 TASK를 소개한 논문
    &lt;ul&gt;
      &lt;li&gt;DATASETS&lt;/li&gt;
      &lt;li&gt;MUSIC REPRESENTATION&lt;/li&gt;
      &lt;li&gt;EVALUATION METHOD&lt;/li&gt;
      &lt;li&gt;challenges &amp;amp; future directions&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;intro&quot;&gt;Intro&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;음악은 언어와 같다.&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;악보 : 음악  == 글 : 말&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Music Generation의 대분류 3개
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/33ed8e71-a0d8-4853-9c90-c9f1940900dc/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;score generation&lt;/li&gt;
      &lt;li&gt;performance generation add performance characteristics to scores
        &lt;ul&gt;
          &lt;li&gt;RENDERING performance&lt;/li&gt;
          &lt;li&gt;composing performance&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;audio generation : convert score with performance characteristics into audio
        &lt;ul&gt;
          &lt;li&gt;assign timbre&lt;/li&gt;
          &lt;li&gt;directry generate music in audio&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;그 중에서도 SCORE GENERATION이 가장 핫함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;style transfer 적용
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/3a730a16-e14a-4174-8d27-05807196346f/image.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;history&quot;&gt;History&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Iliac Suite : computer 로 만들어진 최초의 score
    &lt;ul&gt;
      &lt;li&gt;Markov chain 활용(Stochastic Model)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Coconet&lt;/strong&gt;
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/5b0df87e-0c0b-48b7-892a-b8958dd41752/image.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/e5825a0d-c3fd-450f-91f1-d034fe4c5c91/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;4마디 단위로 잘라서 멜로디에 alto,tenor,bass에 대응하는 멜로디 추가&lt;/li&gt;
      &lt;li&gt;piano roll type의 input을 one-hot vector로 encoding&lt;/li&gt;
      &lt;li&gt;16분음표로 quantization&lt;/li&gt;
      &lt;li&gt;전형적인 딥러닝의 multiclassification task&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Todd’s &lt;strong&gt;Time-Windowed&lt;/strong&gt; and conditioned recurrent architectures
    &lt;ol&gt;
      &lt;li&gt;인공신경망을 어떻게 음악생성에 활용할 수 있을지를 첫번쨰로 보여준 사례
        &lt;ol&gt;
          &lt;li&gt;input : context와 plan으로 나뉨
            &lt;ul&gt;
              &lt;li&gt;context :memory
      - bos + 각 note에 부합하는 unit으로 구성&lt;/li&gt;
              &lt;li&gt;plan : network가 학습한 특정 melody&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;학습방법
 &lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/78d3fb95-60f1-4f67-9b53-088ade4f5879/image.png&quot; alt=&quot;&quot; /&gt;
 1) plan에서 melody 와
 2) 초기화된 context의 첫번쨰 시점으로부터
 3) 첫번쨰 output을 냄 (비교 및 weight update)
 4) output은 다음 time step의 memory로 들어감
 -&amp;gt; iterative하게 output을 기억하여 생성!&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Lewis Creation by refinement&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;traditional-music-generation&quot;&gt;Traditional Music Generation&lt;/h3&gt;
&lt;p&gt;1) rule-based
    - linguistic &amp;amp; grammar 활용
    - 음악마다 다른 rule 이 만들어져야 하는 단점
    - 
2) Probability model&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;semi-automatic 한 알고리즘에 Markov chain이나 이외의 기술을 태움&lt;/li&gt;
  &lt;li&gt;원래의 data로부터 subsequence를 생성하지만, memory가 없어서 input에 따라 조건부 확률이 변동이 심함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3) Neural Network(RNN)
    - RNN으로 feature를 학습하여 단선율의 pitch와 duration을 학습
    - Gradient Vanishing으로 long-term dependency 학습 못하는 단점&lt;/p&gt;

&lt;h3 id=&quot;evloutionary&quot;&gt;evloutionary&lt;/h3&gt;
&lt;p&gt;1) GenJam
2) Director Musices
3) Probablistic&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;hierarchical HMM&lt;/li&gt;
  &lt;li&gt;dynamic Bayesian networks&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sound-modelling&quot;&gt;sound modelling&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;physical modeling&lt;/li&gt;
  &lt;li&gt;acoustic modeling : signal generator 이용
    &lt;ul&gt;
      &lt;li&gt;oscillator, modulator, filters와 같이 waveform 조작을 위한 processors&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lyrics&quot;&gt;lyrics&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;가사 포함 여부에 따라 music audio를 audio와 singing voice로 나눔.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-music-generation&quot;&gt;Deep Music Generation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Google Magenta : &lt;a href=&quot;https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn&quot;&gt;Melody RNN model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Anticipation-RNN : 사용자가 정의한 positional한 제약을 강요함.&lt;/li&gt;
  &lt;li&gt;TP-LSTM-NADE and BALSTM : 병렬 RNN으로부터 다선율의 데이터셋에서 translation-invariance를 유지&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;score-generation-vae-gan-transformer-&quot;&gt;Score Generation: VAE, GAN, Transformer, …&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;MusicVAE : longterm dependency, interpolation &amp;amp; reconstruction 퍼포먼스 굳
    &lt;ul&gt;
      &lt;li&gt;coupled latent variable model&lt;/li&gt;
      &lt;li&gt;binary regularizer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GAN
    &lt;ul&gt;
      &lt;li&gt;sequential data에는 학습에 어려움이 있음&lt;/li&gt;
      &lt;li&gt;CNN based GAN으로 개발&lt;/li&gt;
      &lt;li&gt;GAN-based MidiNet : Chord 에 condition되어 마디를 생성하는 알고리즘&lt;/li&gt;
      &lt;li&gt;MuseGAN : multi-track 다선율 음악을 생성하는 최초의 모델
        &lt;ul&gt;
          &lt;li&gt;강화학습을 접목하여 RNN-based GAN 적용&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transformer
    &lt;ul&gt;
      &lt;li&gt;REMI&lt;/li&gt;
      &lt;li&gt;Transformer XL&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;performance-generation--대부분-rnn&quot;&gt;Performance Generation : 대부분 RNN&lt;/h2&gt;

&lt;h2 id=&quot;representation&quot;&gt;Representation&lt;/h2&gt;
&lt;p&gt;1) symbol domain&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;discrete variable
2) audio domain&lt;/li&gt;
  &lt;li&gt;continuous variable&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;From artificial neural networks to deep learning for music generation: history, concepts and trends&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/generating-music-with-artificial-intelligence-9ce3c9eef806&lt;/li&gt;
  &lt;li&gt;https://topten.ai/music-generators-review/&lt;/li&gt;
&lt;/ul&gt;</content><author><name>건너별(crosstar)</name></author><category term="nlp" /><summary type="html">Abstract 딥러닝을 활용한 최근의 다양한 음악 작곡 TASK를 소개한 논문 DATASETS MUSIC REPRESENTATION EVALUATION METHOD challenges &amp;amp; future directions</summary></entry><entry><title type="html">Paper Review - Identifying and attacking the saddle point problem in high-dimensional non-convex optimization</title><link href="https://crosstar1228.github.io/local_minima_saddle_point" rel="alternate" type="text/html" title="Paper Review - Identifying and attacking the saddle point problem in high-dimensional non-convex optimization" /><published>2022-08-11T11:00:00+00:00</published><updated>2022-08-11T11:00:00+00:00</updated><id>https://crosstar1228.github.io/local_minima_saddle_point</id><content type="html" xml:base="https://crosstar1228.github.io/local_minima_saddle_point">&lt;blockquote&gt;
  &lt;p&gt;Keyword : Saddle Point, SFN(Saddle-Free Newton Method) SGD&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/3666f71f-bc42-4c8a-baf5-4ca4cb767c30/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;1) 고차원의 non-convex function에서는 포괄적으로(generically) saddle point가 문제되는 경우가 많다
2) 고차원의 non-convex function에서는 대체로 local minima 문제는 통계적/경험적으로 매우 드문 경우이다
3) 이에 따라 일반화된 trust region 방법이 개발됨
   1) 도함수가 아닌 신뢰 지역(trust region)의 모양을 감지
   2) 곡률 정보를 알 수 있음&lt;/p&gt;
&lt;h3 id=&quot;saddle-point-에서-벗어나기&quot;&gt;Saddle point 에서 벗어나기&lt;/h3&gt;
&lt;p&gt;4) Saddle-Free Method : Hessian 값의 역으로 절댓값의 gradient value를 재조정
    - 이 방법으로 Gradient Descent와 Newton Method를 적절이 섞어 사용함으로서 빠르게 saddle point에서 벗어날 수 있음&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://papers.nips.cc/paper/2014/file/17e23e50bedc63b4095e3d8204ce063b-Paper.pdf&lt;/li&gt;
&lt;/ul&gt;</content><author><name>건너별(crosstar)</name></author><category term="ML" /><summary type="html">Keyword : Saddle Point, SFN(Saddle-Free Newton Method) SGD</summary></entry><entry><title type="html">Generative Model의 신흥강자, Diffusion Model!</title><link href="https://crosstar1228.github.io/ML-Diffusion_Model" rel="alternate" type="text/html" title="Generative Model의 신흥강자, Diffusion Model!" /><published>2022-07-29T11:00:00+00:00</published><updated>2022-07-29T11:00:00+00:00</updated><id>https://crosstar1228.github.io/ML-Diffusion_Model</id><content type="html" xml:base="https://crosstar1228.github.io/ML-Diffusion_Model">&lt;blockquote&gt;
  &lt;p&gt;이번 시간에 제가 소개해볼 논문이자 인공지능 모델은, 2021년 NeurIPS(신겅정보처리시스템학회) 에서 발표한 모델이자 생성모델 분야의 뜨거운 감자, &lt;strong&gt;Diffusion Model&lt;/strong&gt;입니다!
 이름이 왜 Diffusion Model인지, 모델 구조와 최적화 Metric을 유념하면서 AI 생성모델 분야에 어떠한 영향력을 미치고 있는지 한번 알아보도록 해요!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;generative-model&quot;&gt;Generative Model?&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;주어진 관측 데이터 x 로부터 추출된 분포(distribution)을 평가하는 모델! Autoregressive Model부터 VAE, GAN, Flow - Based Model 등 다양!&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;diffusion-model이어야-하는-이유&quot;&gt;Diffusion model이어야 하는 이유?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;2021 NeurIPS 학회에서 Autoregressive 계열 생성모델보다 likelihood 성능이 좋고, GAN Based model보다 quality가 높은 sample 을 생성하는 것으로 발표되었답니다!&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://paperswithcode.com/paper/diffusion-models-beat-gans-on-image-synthesis&quot;&gt;Diffusion Models Beat GANs on Image Synthesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;diffusion-model&quot;&gt;Diffusion Model&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;점진적인 noise 추가의 역과정을 학습하는 것이 개념의 핵심&lt;/li&gt;
  &lt;li&gt;주어진 데이터 x 로부터 noise를 추가하고, noise data로부터 x로 돌아오는 과정을 학습함&lt;/li&gt;
  &lt;li&gt;VAE와 컨셉은 유사하지만, data 분포를 학습하는 것이 아니라 Markov Chain 안에서 noise 분포를 모델링하는 Latent variable model
    &lt;ul&gt;
      &lt;li&gt;여기서 markov chain이란, 특정한 확률적인 규칙에 의해 하나의 상태에서 다른 특정한 상태로 변화하는 수학적 계(界)를 의미함. 쉽게 말해, 확률에 의한 시간에 따른 상태 변화.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;prior로부터 posterior를 얻기 위하여 x1,x2, xT 의 각 time point 별 latent varible에 noise를 더해가는 구조
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/30bdb629-19dc-4948-a4fb-59d25fd09ddd/image.png&quot; alt=&quot;&quot; /&gt;
noise를 추가하는 과정을 diffusion(확산 또는 전파)라고 이해할 수 있겠죠. 
 $x_T$ 로부터 시간에 따라 조건부 확률에 의해 noise로부터 원래 이미지를 복원해나가는 모습입니다.&lt;/li&gt;
  &lt;li&gt;계충적(hierachical)한 방법으로 data를 denoise, 다시 말해 decode를 하는 것이죠.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;process&quot;&gt;process&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/2917dd77-3028-43f5-9348-5f2ea581f471/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;forward process&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;점진적으로 noise를 더해가는 과정 $q$&lt;/li&gt;
      &lt;li&gt;real data의 distribution을 $q(x_0)$라고 한다면, sampling을 할 수 있음. ($x_0$ ~ $q(x_0)$)
        &lt;ul&gt;
          &lt;li&gt;time step 별로 gaussian noise를 추가하는 과정은 아래와 같다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$q(x_t|x_{t-1}) = N(x_t ; \sqrt{1-\beta_t}x_{t-1}, \beta_tI)$&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;여기서 $\beta$ 는 variance schedule(&lt;strong&gt;학습되기도 하고(parameter), 고정되기도 함(hyperparameter)&lt;/strong&gt;)이라고 해서, 직관적으로는 time step별로 추가될  noise의 분산에 해당하는 값이다.&lt;/li&gt;
      &lt;li&gt;diffusion model을 하나의 함수라고 한다면, model은 noisy component를 $\epsilon(x_t, t)$
        &lt;ul&gt;
          &lt;li&gt;true noise 와 predicted noise의 차이&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;reverse process&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;학습된&lt;/strong&gt; denoising process $p_\theta$&lt;/li&gt;
      &lt;li&gt;원래 $p(x_{t-1}|x_t)$ 의 conditional distribution에 가까운 $p_\theta(x_{t-1}|x_t)$ 를 학습하는 것!
        &lt;ul&gt;
          &lt;li&gt;신경망이 위에 해당하는 parameter를 학습하고 loss 파악하고 gradient descent 로 update하는 과정!&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;역과정도 Gaussian noise를 가정하므로, 정규분포의 평균과 분산이 parameter로 정의됨
        &lt;ul&gt;
          &lt;li&gt;$p_\theta(x_{t-1}|x_t) = N(x_{t_1}; \mu_\theta(x_t, t), \sigma_\theta(x_t, t))$&lt;/li&gt;
          &lt;li&gt;허나 DDPM저자는 variance를 고정시키고, 조건부 확률 분포의 &lt;strong&gt;평균&lt;/strong&gt;만 학습하게 함. 비슷한 결과를 보이기 때문. (이후 더 발전된 형태에서는 variance만 학습)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;denoising 과정이 학습이 되어있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;반복-과정&quot;&gt;반복 과정!&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/6681b933-1134-44f6-abcd-3602ddde8ba1/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;benefits&quot;&gt;Benefits&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;scalability&lt;/li&gt;
  &lt;li&gt;parallelizabliity&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;objective-function&quot;&gt;objective function&lt;/h3&gt;
&lt;p&gt;negative log likelihood를 사용하며, 초기 이미지(ground truth)의 확률분포와 복원된 이미지의 확률분포 간 KL Divergence&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;둘다 Gaussian distribution&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;각 time step의 Loss 를 모두 합한 것이 최종 loss가 됨&lt;/p&gt;

&lt;h3 id=&quot;variance-schedule이-학습-가능한-경우&quot;&gt;variance schedule이 학습 가능한 경우&lt;/h3&gt;
&lt;p&gt;각 time step의 loss 는 아래와 같이 정의되고,
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/dd20c074-9cd6-44d6-8134-9e32e6743f0f/image.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;kl-divergence&quot;&gt;KL divergence?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;쉽게 이야기해서, 확률분포 간의 어긋난 정도
&lt;img src=&quot;https://velog.velcdn.com/images/crosstar1228/post/562b198d-c1ea-4cd6-968d-a5e4ff685a95/image.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;variance-schedule을-고정시키면&quot;&gt;variance schedule을 고정시키면!&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;schedule을 고정시켜도 성능이 충분히 좋다&lt;/li&gt;
  &lt;li&gt;선형 나열보다는 기하학적 나열이 더 성능이 좋음&lt;/li&gt;
  &lt;li&gt;$L_{t-1}$ = $|| \epsilon - \epsilon_\theta(x_t, t) ||^2$&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;h3 id=&quot;정리-및-마무리&quot;&gt;정리 및 마무리&lt;/h3&gt;
  &lt;ul&gt;
    &lt;li&gt;forward process에서 markov chain에 의한 gaussian noise를 추가하고, 특정 variance schedule을 따름&lt;/li&gt;
    &lt;li&gt;reverse process에서 parameter에 의해 원본 복원과정 학습&lt;/li&gt;
    &lt;li&gt;모델 평가는, step별 loss의 합.&lt;/li&gt;
    &lt;li&gt;특정 time step의 복원된 확률분포 $p$ 와 생성된 확률분포 $q$ 간의 차이를 비교함. KL Divregence 또는 gaussian distribution에 대응되는 noise간의 rmse score&lt;/li&gt;
    &lt;li&gt;유연성이 좋고 다양한 모델에 적용 가능. 성능도 좋음 (ex, DALLE-2)&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/pdf/2006.11239.pdf&lt;/li&gt;
  &lt;li&gt;https://www.lgresearch.ai/kor/blog/view/?seq=190&amp;amp;page=1&amp;amp;pageSize=12&lt;/li&gt;
&lt;/ul&gt;</content><author><name>건너별(crosstar)</name></author><category term="ML" /><summary type="html">이번 시간에 제가 소개해볼 논문이자 인공지능 모델은, 2021년 NeurIPS(신겅정보처리시스템학회) 에서 발표한 모델이자 생성모델 분야의 뜨거운 감자, Diffusion Model입니다! 이름이 왜 Diffusion Model인지, 모델 구조와 최적화 Metric을 유념하면서 AI 생성모델 분야에 어떠한 영향력을 미치고 있는지 한번 알아보도록 해요!</summary></entry><entry><title type="html">Music Generation and AI, present and future</title><link href="https://crosstar1228.github.io/NLP-music_and_ai" rel="alternate" type="text/html" title="Music Generation and AI, present and future" /><published>2022-07-18T11:00:00+00:00</published><updated>2022-07-18T11:00:00+00:00</updated><id>https://crosstar1228.github.io/NLP-music_and_ai</id><content type="html" xml:base="https://crosstar1228.github.io/NLP-music_and_ai">&lt;h2 id=&quot;목차&quot;&gt;목차&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://velog.io/@tobigsvoice1516/5%EC%A3%BC%EC%B0%A8-MUSIC-COMPOSITION-WITH-DEEP-LEARNING-A-REVIEW&quot;&gt;music &amp;amp; ai 역사&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openai.com/blog/jukebox/&quot;&gt;OpenAI - JukeBox&lt;/a&gt; [&lt;a href=&quot;https://github.com/openai/jukebox/&quot;&gt;Github&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;Datasets&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ai-music-genration의-시초&quot;&gt;AI Music Genration의 시초&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;90년대 David Bowie 의 Verbasizer (앱)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;단어를 임의로 재배치하여 음악 가사에 사용될 수 있도록 재조합하는 앱이었음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;2016년 Sony의 App Flow Machine
    &lt;ul&gt;
      &lt;li&gt;비틀즈 스타일 멜로디를 창조해 냄&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;music-generation-도-크게-다르지-않아&quot;&gt;Music Generation 도 크게 다르지 않아&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;머신러닝에서 모델은 다량의 데이터를 학습하고 그 안에서 ‘패턴’을 찾아냅니다.&lt;/li&gt;
  &lt;li&gt;Music Generation에서는 그 패턴이 Chord, Tempo, lengths, note 간 관계성 등 이됩니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;symbolic-approach-non-symbolic-approach&quot;&gt;Symbolic approach, Non-symbolic approach&lt;/h3&gt;

&lt;h2 id=&quot;music-generation의-고질적인-문제-1--long-term-dependency&quot;&gt;Music Generation의 고질적인 문제 1 : LONG TERM DEPENDENCY&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;해결법 1 : autoencoder로 저차원 space로 mapping
    &lt;ul&gt;
      &lt;li&gt;불필요한 정보를 버리게 됨&lt;/li&gt;
      &lt;li&gt;이후 upsampling&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MuseNet : midi data 기반 많은 양의 데이터 학습&lt;/li&gt;
  &lt;li&gt;Transfomer 계열 모델로 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;아이디어-&quot;&gt;아이디어 :&lt;/h3&gt;
&lt;p&gt;o learn a lower-dimensional encoding of the audio with the goal of losing the less important
information but retaining most of the musical information&lt;/p&gt;
&lt;h2 id=&quot;문제-2--diversityvariation&quot;&gt;문제 2 : Diversity(variation)&lt;/h2&gt;

&lt;h3 id=&quot;jukeboxpaper&quot;&gt;JukeBox[&lt;a href=&quot;https://arxiv.org/abs/2005.00341&quot;&gt;Paper&lt;/a&gt;]&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;long context 를 autoregressiveTransformer 이용한 multi-sclae VQ-VAE로 해결&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lyric-conditioning&quot;&gt;Lyric Conditioning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;노래의 duration에 linear 하게 가사의 문자들을 align하는 방법&lt;/li&gt;
  &lt;li&gt;가사를 위한 encoder를 더하고, &lt;strong&gt;music decoder로부터 의 query&lt;/strong&gt;로부터 &lt;strong&gt;가사 encoder로부터의 key, value 쌍&lt;/strong&gt; 으로의 attetion layer를 적용함.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vq-vae-codebook-collapse&quot;&gt;VQ-VAE codebook collapse&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;codebook에 mapping된 embedding vector들이 많이 쓰이지 않는 현상&lt;/li&gt;
  &lt;li&gt;Random Restart:codebook vector 사용량이 평균이하로 떨어지면 , encoder output 중 하나로 다시 reset&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;https://magenta.tensorflow.org/perceiver-ar&lt;/p&gt;

&lt;h3 id=&quot;sparse-transformer&quot;&gt;Sparse Transformer&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;sparsifies the attention pattern by
reshaping the input sequence into a &lt;strong&gt;2-D sequence&lt;/strong&gt; of
shape&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;google-deepmind-202206&quot;&gt;Google Deepmind (2022.06)&lt;/h2&gt;
&lt;p&gt;[&lt;a href=&quot;https://arxiv.org/abs/2202.07765&quot;&gt;Paper&lt;/a&gt;]&lt;/p&gt;
&lt;h3 id=&quot;perceiver-ar&quot;&gt;Perceiver AR&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;modality 에 대하여 agnostic(인지불능)인 구조
    &lt;ul&gt;
      &lt;li&gt;cross attention : long-range input -&amp;gt; small latent&lt;/li&gt;
      &lt;li&gt;maintaining end-to-end causal masking
https://soundraw.io/ 
https://magenta.tensorflow.org/
https://www.aiva.ai/
-&amp;gt; 음악 작곡&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Datasets&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://paperswithcode.com/task/music-generation&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;music-generation-and-deep-learning&quot;&gt;Music Generation and Deep learning&lt;/h2&gt;
&lt;p&gt;1) 딥러닝 베이스 음악 생성의 컨셉
2) 음악 생성의 다양한 방법과 원리
3) 다양한 음악 생성의 개념적 분류 체계
4) 트렌드&lt;/p&gt;

&lt;p&gt;abstract model이 generation을 위해 사용됨&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sota models
MuseGAN
Melnet
MidiNet&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;From artificial neural networks to deep learning for music generation: history, concepts and trends&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/generating-music-with-artificial-intelligence-9ce3c9eef806&lt;/li&gt;
  &lt;li&gt;https://topten.ai/music-generators-review/&lt;/li&gt;
&lt;/ul&gt;</content><author><name>건너별(crosstar)</name></author><category term="nlp" /><summary type="html">목차 music &amp;amp; ai 역사 OpenAI - JukeBox [Github] Datasets</summary></entry><entry><title type="html">AWS와 인프라 1주차 aaS, Cloud, Storage 란 무엇인가</title><link href="https://crosstar1228.github.io/MLOps-week1" rel="alternate" type="text/html" title="AWS와 인프라 1주차 aaS, Cloud, Storage 란 무엇인가" /><published>2021-10-17T16:40:00+00:00</published><updated>2021-10-17T16:40:00+00:00</updated><id>https://crosstar1228.github.io/MLOps-week1</id><content type="html" xml:base="https://crosstar1228.github.io/MLOps-week1">&lt;h2 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#iaas-paas-saas&quot;&gt;IaaS, PaaS, SaaS&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#iaas&quot;&gt;IaaS&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#paas&quot;&gt;PaaS&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#saas&quot;&gt;SaaS&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#public-cloud-private-cloud-hybrid-cloud&quot;&gt;Public Cloud, Private Cloud, Hybrid Cloud&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#public-cloud&quot;&gt;public cloud&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#private-cloud&quot;&gt;private cloud&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#hybrid-cloud&quot;&gt;Hybrid Cloud&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#das-san-nas-storage&quot;&gt;DAS, SAN, NAS (Storage)&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#das&quot;&gt;DAS&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#san&quot;&gt;SAN&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nas&quot;&gt;NAS&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;용어정리&quot;&gt;용어정리&lt;/h2&gt;
&lt;p&gt;Server(서버) : 컴퓨터 프로그램 또는 장치.  네트워크를 통해 클라이언트로부터의 정보를 받아 처리 및 응답하는 서비스를 제공하는 컴퓨터 시스템
Storage(스토리지) : 쉽게 말해 저장소를 의미
Network(네트뭐크) : 연결을 통해 컴퓨터 자원 공유하는 것, 또는 그런 체계
Web(웹) : 인터넷의 한 종류로서, 인터넷에 연결된 사용자들이 서로의 정보를 공유할 수 있는 공간.
third-party(서드파티) : 클라우드 서비스를 제공하는 제 3사를 의미합니다.
middleware(미들웨어) : 양 쪽을 연결하여 데이터를 주고받을 수 있도록 중간에서 매개 역할을 하는 소프트웨어 ex) 웹브라우저로부터 데이터를 저장할 수 있게 해주는 DB시스템
hosting(호스팅) : 제공자등의 사업자가 개인용 홈페이지의 서버 기능을 대행하는 것. 또 기업의 대용량 메모리 공간을 이용하여 사용자의 홈피나 웹 서버 기능을 대행하는 서비스.
TCO(Total Cost of Ownership) : 서버 도입 및 유지/보수에 들어가는 컴퓨팅 시스템의 총비용.&lt;/p&gt;

&lt;h2 id=&quot;iaas-paas-saas&quot;&gt;IaaS, PaaS, SaaS&lt;/h2&gt;
&lt;p&gt;aaS : as-a-Service 를 의미하며, 클라우드 기반 서비스를 지칭할 때 쓰이는 말입니다.&lt;/p&gt;

&lt;h3 id=&quot;iaas&quot;&gt;IaaS&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;infrastructure&lt;/strong&gt;-as-a-Service 를 의미하며, 서드파티 업체가 제공하는 고도로 자동화되고 확장 가능한 인프라를 의미합니다.
이 인프라에서는 스토리지, 호스팅, 컴퓨팅, 네트워킹 등이 포함되어 있고, 비용은 사용한 만큼만 지급하게 됩니다.
따라서 기업은 IaaS 를 통하여 IT자산(소프트웨어 서버, 라이센스)등을 직접 소유하는 대신
필요에 따라 리소스를 유연하게 대여할 수 있습니다.
&lt;strong&gt;AWS&lt;/strong&gt; 가 이 시장을 40% 점유하고 있다는 점이 주목할 만합니다.&lt;/p&gt;
&lt;h3 id=&quot;paas&quot;&gt;PaaS&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Platform&lt;/strong&gt;-as-a-service 를 의미하며, &lt;strong&gt;IaaS + 개발툴과 기능, 앱배포 등의 플랫폼 전반적인 영역&lt;/strong&gt;을 제공하기에 가장 까다로운 영역입니다.
반대로 서비스를 이용하는 개발자의 입장에서는, 기반 infrastructure를 provisioning 할  필요가 없어집니다.
주로 대형 IT기업에서 볼 수 있고, 구글 앱엔진, 오라클의 클라우드 플랫폼 등이 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;saas&quot;&gt;SaaS&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Software&lt;/strong&gt;-as-a-service 를 의미하며, Third party가 &lt;strong&gt;hosting 방식&lt;/strong&gt;으로 소프트웨어를 제공하는 것을 지칭합니다. &lt;strong&gt;웹을 통해 로그인하면 사용&lt;/strong&gt;할 수 있고, &lt;strong&gt;구독 형식&lt;/strong&gt;으로 과금되는 것이 일반적입니다.
특장점은 머신 혹은 서버를 기준으로 소프트웨어 라이센스를 구매하기 때문에, 설치할 필요 없이 웹에서 사용이 가능합니다.
필요할 때 비용만 내면 얼마든지 사용이 가능하며, 사용자가 일일이 패치, 업그레이드 할 필요가 없다는 것도 장점입니다.
ex) 이메일, CRM software, 구글 독스&lt;/p&gt;

&lt;h3 id=&quot;정리&quot;&gt;정리&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;SaaS -&amp;gt; IaaS -&amp;gt; PaaS 로 서비스 자원 제공 범위가 확장되는 개념&lt;/li&gt;
  &lt;li&gt;최초 Cloud 서비스는 지메일, 드롭박스, 네이버 클라우드 처럼 Software 를 App에서 쓸 수 있는 SaaS 가 대부분이었음&lt;/li&gt;
  &lt;li&gt;이후 서버와 스토리지, 네트워크 같은 인프라 장비를 빌려주는 IaaS, 그리고 플랫폼을 빌려주는 PaaS로 발전&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;public-cloud-private-cloud-hybrid-cloud&quot;&gt;Public Cloud, Private Cloud, Hybrid Cloud&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/built/images/infra_week1_img.png&quot; alt=&quot;img.png&quot; /&gt;
출처 : https://www.bmc.com/blogs/public-private-hybrid-cloud/#&lt;/p&gt;
&lt;h3 id=&quot;public-cloud&quot;&gt;Public Cloud&lt;/h3&gt;
&lt;p&gt;장점&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;인터넷을 통해 전달되고 조직 간에 공유가 가능한 저장소&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;유지비용 없음&lt;/li&gt;
  &lt;li&gt;복잡하지 않고 flexible함
단점&lt;/li&gt;
  &lt;li&gt;보안의 문제가 있음&lt;/li&gt;
  &lt;li&gt;customizing이 어려움
    &lt;h3 id=&quot;private-cloud&quot;&gt;Private Cloud&lt;/h3&gt;
  &lt;/li&gt;
  &lt;li&gt;말 그대로 &lt;strong&gt;개인 저장소&lt;/strong&gt;
장점&lt;/li&gt;
  &lt;li&gt;customizing가능&lt;/li&gt;
  &lt;li&gt;효율적이고 보안에 강함&lt;/li&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;hybrid-cloud&quot;&gt;Hybrid Cloud&lt;/h3&gt;
  &lt;/li&gt;
  &lt;li&gt;public 과 private 을 둘 다 쓰는 환경&lt;/li&gt;
  &lt;li&gt;각각의 장단점이 보완된 형태&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;das-san-nas-storage&quot;&gt;DAS, SAN, NAS (Storage)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Storage system&lt;/strong&gt; 이란?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;단일 디스크로 처리할 수 없는 용량을 저장하기 위해 디스크를 묶어서 논리적으로 사용하는 기슬&lt;/li&gt;
  &lt;li&gt;데이터 범람으로 인해 효율적인 저장과 관리에 대한 수요가 급증하였고, 정보 자원을 저장하는 방법론은 하나의 기술로서 자리 잡게 되었음
&lt;img src=&quot;../../assets/built/images/infra_week1_img_1.png&quot; alt=&quot;img_1.png&quot; /&gt;
    &lt;h3 id=&quot;das&quot;&gt;DAS&lt;/h3&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Direct Attached Storage&lt;/strong&gt;의 약자로, 시스템에 직접 붙이는 외장 storage를 의미(외장하드 등)&lt;/li&gt;
  &lt;li&gt;전용 연결장치가 있으므로 NAS 보다는 Access 속도가 빠르지만 시스템에 1대1로밖에 적용이 안되는 단점
    &lt;h3 id=&quot;san&quot;&gt;SAN&lt;/h3&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Storage Area Network&lt;/strong&gt; 의 약자로, 서로 다른 종류의 저장장치를 관련 데이터 서버와 함께 연결해 별도의 랜(근거리통신망)이나 네트워크를 구성해 저장공간 관리&lt;/li&gt;
  &lt;li&gt;서로 다른 저장장치가 함께 연결되어 있어서 모든 사용자들이 공유 가능&lt;/li&gt;
  &lt;li&gt;백업, 복원, 영구보관 및 검색이 가능하고 한 저장장치에서 다른 저장장치로 데이터를 이동시킬 수 있다는 장점이 있다&lt;/li&gt;
  &lt;li&gt;별도의 네트워크 서버를 구축해야 한다는 단점이 있음
    &lt;h3 id=&quot;nas&quot;&gt;NAS&lt;/h3&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Network Attached Storage&lt;/strong&gt;의 약자로, 파일서버의 한계를 극복한 파일공유를 위한 전통적 솔루션&lt;/li&gt;
  &lt;li&gt;네트워크에 붙어있기 때문에 셋팅이 쉬움&lt;/li&gt;
  &lt;li&gt;파일 공유에 큰 장점 - 파일시스템 공유 가능&lt;/li&gt;
  &lt;li&gt;LAN과 채널 속도에 성능이 좌우된다는 단점이 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;정리-1&quot;&gt;정리&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;DAS-NAS-SAN 순으로 점진적으로 확장되는 개념&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;1) aaS : https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&amp;amp;blogId=futuremain&amp;amp;logNo=221360648846
2) DAS, SAN, NAS : https://blog.naver.com/gojump0713/140111144418&lt;/p&gt;</content><author><name>건너별(crosstar)</name></author><category term="MLOps" /><summary type="html">Table of Contents IaaS, PaaS, SaaS IaaS PaaS SaaS Public Cloud, Private Cloud, Hybrid Cloud public cloud private cloud Hybrid Cloud DAS, SAN, NAS (Storage) DAS SAN NAS</summary></entry><entry><title type="html">모델 서빙과 MLOps</title><link href="https://crosstar1228.github.io/MLOps-week2" rel="alternate" type="text/html" title="모델 서빙과 MLOps" /><published>2021-10-17T16:40:00+00:00</published><updated>2021-10-17T16:40:00+00:00</updated><id>https://crosstar1228.github.io/MLOps-week2</id><content type="html" xml:base="https://crosstar1228.github.io/MLOps-week2">&lt;h2 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#iaas-paas-saas&quot;&gt;IaaS, PaaS, SaaS&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#iaas&quot;&gt;IaaS&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#paas&quot;&gt;PaaS&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#saas&quot;&gt;SaaS&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#public-cloud-private-cloud-hybrid-cloud&quot;&gt;Public Cloud, Private Cloud, Hybrid Cloud&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#public-cloud&quot;&gt;public cloud&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#private-cloud&quot;&gt;private cloud&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#hybrid-cloud&quot;&gt;Hybrid Cloud&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#das-san-nas-storage&quot;&gt;DAS, SAN, NAS (Storage)&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#das&quot;&gt;DAS&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#san&quot;&gt;SAN&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nas&quot;&gt;NAS&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;모델-서빙이란&quot;&gt;모델 서빙이란?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;머신러닝 모델의 예측값을 실제로 사용자에게 전달하는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mlops의-필요성&quot;&gt;MLOps의 필요성&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;모델의 성능은 test data의 추이에 따라 끊임없이 변함&lt;/li&gt;
  &lt;li&gt;새롭게 개발한 모델이 더 성능이 안좋을 수도 있음(애매하거나)&lt;/li&gt;
  &lt;li&gt;그럼 이전 모델을 다시 써야야할지, 새 모델을 써볼지 모델의 &lt;strong&gt;형상관리&lt;/strong&gt;가 되어야 함&lt;/li&gt;
  &lt;li&gt;그래서 버전 업데이트로 인해 사용자 경험이 끊기지 않아야 함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;형상-관리&quot;&gt;형상 관리?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;모델의 변경이력과 성능에 대한 정보를 관리하는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;wandb의-필요성&quot;&gt;wandb의 필요성&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;하이퍼 파라미터 몇일 때 최고성능이었나?&lt;/li&gt;
  &lt;li&gt;열심히 돌렸는데 성과가 안나오네.. 인사이트라도 공유해야지&lt;/li&gt;
  &lt;li&gt;어떤 하이퍼파라미터가 제일 중요해?&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/daria-blog/%EB%AA%A8%EB%8D%B8-%EC%84%9C%EB%B9%99%EC%9D%B4%EB%9E%80-21f970e6cfa5&quot;&gt;모델 서빙이란?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>건너별(crosstar)</name></author><category term="MLOps" /><summary type="html">Table of Contents IaaS, PaaS, SaaS IaaS PaaS SaaS Public Cloud, Private Cloud, Hybrid Cloud public cloud private cloud Hybrid Cloud DAS, SAN, NAS (Storage) DAS SAN NAS</summary></entry><entry><title type="html">모델 서빙과 MLOps</title><link href="https://crosstar1228.github.io/MLOps-week3" rel="alternate" type="text/html" title="모델 서빙과 MLOps" /><published>2021-10-17T16:40:00+00:00</published><updated>2021-10-17T16:40:00+00:00</updated><id>https://crosstar1228.github.io/MLOps-week3</id><content type="html" xml:base="https://crosstar1228.github.io/MLOps-week3">&lt;h2 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#iaas-paas-saas&quot;&gt;IaaS, PaaS, SaaS&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#iaas&quot;&gt;IaaS&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#paas&quot;&gt;PaaS&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#saas&quot;&gt;SaaS&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#public-cloud-private-cloud-hybrid-cloud&quot;&gt;Public Cloud, Private Cloud, Hybrid Cloud&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#public-cloud&quot;&gt;public cloud&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#private-cloud&quot;&gt;private cloud&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#hybrid-cloud&quot;&gt;Hybrid Cloud&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#das-san-nas-storage&quot;&gt;DAS, SAN, NAS (Storage)&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#das&quot;&gt;DAS&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#san&quot;&gt;SAN&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nas&quot;&gt;NAS&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;모델-서빙이란&quot;&gt;모델 서빙이란?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;머신러닝 모델의 예측값을 실제로 사용자에게 전달하는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mlops의-필요성&quot;&gt;MLOps의 필요성&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;모델의 성능은 test data의 추이에 따라 끊임없이 변함&lt;/li&gt;
  &lt;li&gt;새롭게 개발한 모델이 더 성능이 안좋을 수도 있음(애매하거나)&lt;/li&gt;
  &lt;li&gt;그럼 이전 모델을 다시 써야야할지, 새 모델을 써볼지 모델의 &lt;strong&gt;형상관리&lt;/strong&gt;가 되어야 함&lt;/li&gt;
  &lt;li&gt;그래서 버전 업데이트로 인해 사용자 경험이 끊기지 않아야 함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;형상-관리&quot;&gt;형상 관리?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;모델의 변경이력과 성능에 대한 정보를 관리하는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/daria-blog/%EB%AA%A8%EB%8D%B8-%EC%84%9C%EB%B9%99%EC%9D%B4%EB%9E%80-21f970e6cfa5&quot;&gt;모델 서빙이란?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>건너별(crosstar)</name></author><category term="MLOps" /><summary type="html">Table of Contents IaaS, PaaS, SaaS IaaS PaaS SaaS Public Cloud, Private Cloud, Hybrid Cloud public cloud private cloud Hybrid Cloud DAS, SAN, NAS (Storage) DAS SAN NAS</summary></entry><entry><title type="html">내용 핵심요약! CS231n Lecture 3. Loss function and Optimization</title><link href="https://crosstar1228.github.io/cs231n-lec3" rel="alternate" type="text/html" title="내용 핵심요약! CS231n Lecture 3. Loss function and Optimization" /><published>2021-10-14T11:00:00+00:00</published><updated>2021-10-14T11:00:00+00:00</updated><id>https://crosstar1228.github.io/cs231n-lec3</id><content type="html" xml:base="https://crosstar1228.github.io/cs231n-lec3">&lt;h1 id=&quot;goal&quot;&gt;GOAL&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;parameter를 Loss function을 통해 update하는 &lt;em&gt;optimization&lt;/em&gt; 과정을 이해합니다.&lt;/li&gt;
  &lt;li&gt;Gradient descent 과정을 개괄적으로 이해합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#loss-function&quot;&gt;Loss function&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#multiclass-svm-loss&quot;&gt;Multiclass SVM Loss&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#regularization&quot;&gt;Regularization&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#Softmax-function&quot;&gt;Softmax function&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#optimization&quot;&gt;Optimization&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#stochastic-gradient-descent&quot;&gt;Stochastic Gradient Descent&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;loss-function&quot;&gt;Loss function&lt;/h2&gt;
&lt;p&gt;지난 시간에 우리는 Parameter를 update, 즉 최적의 가중치를 찾아내는 방법에 대한 필요성을 느꼈습니다.
한 마디로 이번 강의에서 설명할 최적화(Optimization)에 대한 이야기입니다. 그리고 그를 위한 핵심 개념인 손실 함수(Loss function)에 대하여 우선적으로 알아보겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;what-is-loss-function&quot;&gt;What is loss function?&lt;/h3&gt;
&lt;p&gt;Loss function(손실 함수, 이하 한글 용어 생략)은 다른 말로 Cost function이라고 합니다.
Loss 또는 cost를 계산하는 함수다! 이렇게 예상이 되네요.
그리고 그러한 손실(또는 비용)이 줄어드는 것이 더 바람직한 방향이라는 감까지 오셨다면
Loss function에 대한 직관은 이미 반 정도 익히신 것으로 생각되네요. 조금더 자세히 들어다 볼까요?
&lt;img src=&quot;../../assets/built/images/lec3_img.png&quot; alt=&quot;img.png&quot; /&gt;
우리는 위와 같은 이미지 분류 문제에서 진짜 class(label 또는 정답)에 해당하는 score가 가장 높은 점수가 나오도록 모델을 만들고 싶어요.
그래야만 새로운 이미지도 올바른 정답으로 판단할 확률이 높을 테니까요. 우리는 이것은 정확도(accuracy) 등의 지표로 판단하게 됩니다.
하지만 그림을 보면 고양이의 정답을 가진 사진은 deer(사슴)에 해당하는 가장 높은 점수를 갖고 있고, 개구리는 truck(트럭)에 해당하는 점수를 가장 높게 갖고 있네요.&lt;/p&gt;

&lt;p&gt;수치로 표현된 이러한 점수(score)들이 얼마나 바람직한지, &lt;strong&gt;정량적으로&lt;/strong&gt; 어긋난 정도를 판단할 필요성을 느낍니다.
그 벗어난 정도를 알아야 알맞게 가중치를 갱신할 수 있기 때문입니다.
정량적인 수치로 표현해야 하니, 특정한 입력값마다 변하는 하나의 함수가 정의되는 것이고,
그것이 Loss function으로 표현되는 것입니다.
&lt;img src=&quot;../../assets/built/images/lec3_img_3.png&quot; alt=&quot;img_3.png&quot; /&gt;
&lt;img src=&quot;../../assets/built/images/lec3_img_1.png&quot; alt=&quot;img_1.png&quot; /&gt;
$f(x_i, W)$ : input과 parameter에 의한 예측된 score를 의미합니다.&lt;br /&gt;
$y_i$ : label, 즉 정답에 해당하는 score를 의미합니다. 분류 문제에서는 정답의 class가 1, 나머지는 0으로 기록되어 있습니다.&lt;/p&gt;

&lt;p&gt;두 값의 차이가 N개의 data별로 각각 존재할 것이고, 그것을 평균낸 값을 우리는 $L(W)$로 정의하는 것입니다. 
$W$, 즉 parameter가 독립변수로 존재하는 함수이니, 이 값에 따라 Loss function의 결괏값도 달라지지라는 것을 직관적으로 이해할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 Loss function의 예시인 Multiclass SVM Loss 를 살펴보며 이해도를 높여 보겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;multiclass-svm-loss&quot;&gt;Multiclass SVM Loss&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../../assets/built/images/lec3_img_2.png&quot; alt=&quot;img_2.png&quot; /&gt;
 이전에 편의상 score라고 표현한 예측값이 여기서 정식으로 정의됩니다.
그리고 $L_i$는 다음과 같이 정의돼요!&lt;/p&gt;

&lt;p&gt;$L_i = \underset{j\not=y_i} \sum$
\(\begin{cases}0 \qquad\qquad\qquad if\quad s_{y_i}\geq s_j + 1\\s_j - s_{y_i} + 1\quad \: if\quad otherwise
\end{cases}\)&lt;/p&gt;

&lt;p&gt;다시 쓰자면,&lt;/p&gt;

&lt;p&gt;$L_i = \underset{j\not=y_i} \sum max(0, s_j - s_{y_i} + 1 )$&lt;/p&gt;

&lt;p&gt;$s_j$ : j번째 class score&lt;br /&gt;
$s_{y_i}$ : 정답(label)에 해당하는 score&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;어떤 수식을 이해하는데 최우선인 직관부터 가져가 봅시다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;0과 &lt;strong&gt;특정한 값&lt;/strong&gt;중에서 최대를 골라요. 그 특정한 값이라는 것은&lt;/li&gt;
  &lt;li&gt;(j번째 class score) - (정답에 해당하는 class score) 를 구하고 그것에 1을 더한 값입니다.&lt;/li&gt;
  &lt;li&gt;1은 safety margin이라고 해서, &lt;strong&gt;오답의 score가 정답의 score보다 1 이상 차이나는 정도&lt;/strong&gt;에 한해 Loss로 반영하겠다는 의미입니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;아래 예시를 통해 이해해 봅시다!
&lt;img src=&quot;../../assets/built/images/lec3_img_5.png&quot; alt=&quot;img_5.png&quot; /&gt;
정답 라벨(고양이) 을 제외한 모든 class score를 label에 해당하는 score와의 연산을 통해 2.9 라는 Loss 를 도출해 낸 결과네요.
이러한 연산을 data의 개수만큼 실행 후 평균을 내면 최종적으로 우리가 원하는 Loss값을 알 수 있게 돼요!
&lt;img src=&quot;../../assets/built/images/lec3_img_6.png&quot; alt=&quot;img_6.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이로서 Loss가 0이 되도록 parameter가 얼마나 안좋은지 그 정도를 알고, 더 좋게 만들 수 있게 되었습니다.&lt;/p&gt;

&lt;h3 id=&quot;hinge-loss-관련-질문&quot;&gt;Hinge loss 관련 질문&lt;/h3&gt;
&lt;p&gt;아래의 질문에 대한 답변을 확인해면서 수식을 이해해 보시기 바랍니다!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q1. What happens to the loss if car scores change a bit?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;→ NO CHANGE&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q2.Min/Max of loss&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;→ min = 0, Max = infinity&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q3. At initialization, W is small so all s~0. What is the loss?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;→ n-1 because (n-1) * 1&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q4. What if the sum was over all classes including $s_{y_i}$?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;→ Loss increases by 1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q5. What if we used mean instead of sum ?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;→ Doesn’t Change. We don’t care the true values of the score&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q6. What if we used $max()^2$?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;→ Squared hinge loss. does not used normally&lt;/p&gt;

&lt;p&gt;SVM Loss 아래와 같이 경첩과 같은 모양을 하고 있다고 해서 hinge loss 라고 불리기도 합니다!
&lt;img src=&quot;../../assets/built/images/lec3_img_4.png&quot; alt=&quot;img_4.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;regularization&quot;&gt;Regularization&lt;/h3&gt;
&lt;p&gt;Regularization은 ‘정규화’라고 해석되기도 하나, 개인적으로 한글 용어로 사용을 추천드리지 않습니다.
Normalization도 똑같이 ‘정규화’ 라고 해석된 책들이 많기 때문이고, 두 용어는 완전히 다른 의미를 갖고 있기 때문에 혼동을 야기할 가능성이 다분해요.
그래서 이 용어는 영문 단어 그자체로 의미를 받아들이기로 하고 들어가 봅시다.
&lt;img src=&quot;../../assets/built/images/lec3_img_7.png&quot; alt=&quot;img_7.png&quot; /&gt;
강의에서 설명하는 내용을 간단히 요약하자면, Overfitting을 막기 위해서, 즉 ‘모델의 융통성을 기르기 위해서’ 추가로 Loss function에 더해주는 작업을 의미합니다.
이 요소를 더해주지 않으면, Loss를 최대한 줄이도록 parameter를 설정하게 되었더라도 결국 새로운 test data에는 낮은 예측 정확도를 보이게 될 것입니다.
다시 말해, Loss function을 단순히 줄이는 것만이 능사는 아니라는 것이지요.&lt;/p&gt;

&lt;p&gt;여기서 $\lambda$는 우리가 설정해주어야 할 hyperparameter이며, $R(W)$ 우리가 더해지는 값에 적용할 페널티에요.
본 강의에서는 L2 Regularization을 소개하면서 weight 원소들의 제곱값을 $R(W)$ 로 정의해서 Loss function에 페널티를 주었어요.
페널티를 주었다는 얘기는, $R(W)$ 의 크기가 클수록 모델의 복잡도를 낮추는 방향으로 update하도록 하는 역할을 수행하는 거에요.&lt;br /&gt;
&lt;a href=&quot;https://www.notion.so/Lecture-3-Loss-functions-and-Optimization-3c46c15413324bc7856387118e6cfff1#f4708d3b84444fb7be5d93b71805083b&quot;&gt;L1, L2 Regularization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Regularization에는 L1, L2 regularization 말고도 &lt;strong&gt;Dropout, Batch Normaization&lt;/strong&gt; 등 매우 자주 쓰이며 중요하게 다루어지는 방법들이 있습니다.&lt;/p&gt;

&lt;p&gt;오늘 내용이 조금 어렵다면 괜찮습니다. 다음에 또 다시 보면서 Regularization에 대한 직관을 얻고,
오늘은 &lt;strong&gt;Overfitting을 막기 위한 과정&lt;/strong&gt;는 사실만 우선적으로 기억하고 넘어갑시다!&lt;/p&gt;

&lt;h3 id=&quot;softmax-function&quot;&gt;Softmax function&lt;/h3&gt;
&lt;p&gt;score를 기반으로, log를 포함한 특정한 연산을 진행하고, multiclass에 대한 확률값을 return 받는 또다른 Loss function입니다.
이는 차후 익혀야 할 cross-entropy와도 연관되어 있는 매우 중요한 개념이니 잘 익혀둡시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/lec3_img_8.png&quot; alt=&quot;img_8.png&quot; /&gt;
위 슬라이드에 굉장히 많은 내용이 함축되어 있으니 주의깊게 보시기 바랍니다.&lt;/p&gt;

&lt;h2 id=&quot;optimization&quot;&gt;Optimization&lt;/h2&gt;
&lt;h3 id=&quot;stochastic-gradient-descent&quot;&gt;Stochastic Gradient Descent&lt;/h3&gt;

&lt;h2 id=&quot;질문&quot;&gt;질문&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;softmax function에서 log를 쓰는 이유는 무엇일까요?&lt;/li&gt;
  &lt;li&gt;multiclass SVM Loss 는 왜 SVM Loss라는 이름이 붙었을까요?&lt;/li&gt;
&lt;/ul&gt;</content><author><name>건너별(crosstar)</name></author><category term="ML" /><summary type="html">GOAL parameter를 Loss function을 통해 update하는 optimization 과정을 이해합니다. Gradient descent 과정을 개괄적으로 이해합니다.</summary></entry></feed>